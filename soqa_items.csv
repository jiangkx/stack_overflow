_question,_qv_content,_qv_vote,_qv_comment,total_answer_count,accepted_answer,at_vote,at_comment,suggestedanswers,sv_vote,sv_comment
Improving this Scrapy crawler,"I've written a web crawler which extracts absolute links from the starting URL, and keeps visiting absolute links within the domain until it stops. Scrapy automatically doesn't follow duplicate links. The crawler works. import scrapy class TestSpider(scrapy.Spider): name = 'test_spider' start_urls = ['http://homeguide.ph/'] allowed_domains = ['homeguide.ph'] # for the initial visit def parse(self, response): links = response.xpath('//a/@href').extract() for link in links: if link.find(""#"") == -1: yield scrapy.Request(link, callback=self.parse_link) # for subsequent visits def parse_link(self, response): self.logger.info(""Visited %s"", response.url) links = response.xpath('//a/@href').extract() for link in links: if link.find(""#"") == -1: # visit only absolute links yield scrapy.Request(link, callback=self.parse_link) I feel like it can be improved, though. I'm not sure how. Is there a way to improve this crawler?",-2,,1 Answer 1,,,,1) You can use CrawlSpider object. It extract all links and follow it automatically. You can check it here,2,
Why scrapy crawler stops?,"I have written a crawler using scrapy framework to parse a products site. The crawler stops in between suddenly without completing the full parsing process. I have researched a lot on this and most of the answers indicate that my crawler is being blocked by the website. Is there any mechanism by which I can detect whether my spider is being stopped by website or does it stop on its own? The below is info level log entry of spider . 2013-09-23 09:59:07+0000 [scrapy] INFO: Scrapy 0.18.0 started (bot: crawler) 2013-09-23 09:59:08+0000 [spider] INFO: Spider opened 2013-09-23 09:59:08+0000 [spider] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min) 2013-09-23 10:00:08+0000 [spider] INFO: Crawled 10 pages (at 10 pages/min), scraped 7 items (at 7 items/min) 2013-09-23 10:01:08+0000 [spider] INFO: Crawled 22 pages (at 12 pages/min), scraped 19 items (at 12 items/min) 2013-09-23 10:02:08+0000 [spider] INFO: Crawled 31 pages (at 9 pages/min), scraped 28 items (at 9 items/min) 2013-09-23 10:03:08+0000 [spider] INFO: Crawled 40 pages (at 9 pages/min), scraped 37 items (at 9 items/min) 2013-09-23 10:04:08+0000 [spider] INFO: Crawled 49 pages (at 9 pages/min), scraped 46 items (at 9 items/min) 2013-09-23 10:05:08+0000 [spider] INFO: Crawled 59 pages (at 10 pages/min), scraped 56 items (at 10 items/min) Below is last part of debug level entry in log file before spider is closed: 2013-09-25 11:33:24+0000 [spider] DEBUG: Crawled (200) <GET http://url.html> (referer: http://site_name) 2013-09-25 11:33:24+0000 [spider] DEBUG: Scraped from <200 http://url.html> //scrapped data in json form 2013-09-25 11:33:25+0000 [spider] INFO: Closing spider (finished) 2013-09-25 11:33:25+0000 [spider] INFO: Dumping Scrapy stats: {'downloader/request_bytes': 36754, 'downloader/request_count': 103, 'downloader/request_method_count/GET': 103, 'downloader/response_bytes': 390792, 'downloader/response_count': 103, 'downloader/response_status_count/200': 102, 'downloader/response_status_count/302': 1, 'finish_reason': 'finished', 'finish_time': datetime.datetime(2013, 9, 25, 11, 33, 25, 1359), 'item_scraped_count': 99, 'log_count/DEBUG': 310, 'log_count/INFO': 14, 'request_depth_max': 1, 'response_received_count': 102, 'scheduler/dequeued': 100, 'scheduler/dequeued/disk': 100, 'scheduler/enqueued': 100, 'scheduler/enqueued/disk': 100, 'start_time': datetime.datetime(2013, 9, 25, 11, 23, 3, 869392)} 2013-09-25 11:33:25+0000 [spider] INFO: Spider closed (finished) Still there are pages remaining to be parsed, but the spider stops.",2,The crawler parses few product detail pages and stops in between without parsing all the product details pages.,1 Answer 1,,,,"So far I know that for a spider: There are some queue or pool of urls to be scraped/parsed with parsing methods. You can specify, bind the url to a specific method or let the default 'parse' do the job. From parsing methods you must return/yield another request(s), to feed that pool, or item(s) When the pool runs out of urls or a stop signal is sent the spider stops crawling. Would be nice if you share your spider code so we can check if those binds are correct. It's easy to miss some bindings by mistake using SgmlLinkExtractor for example.",0,I am trying this example mherman.org/blog/2012/11/08/… – Grahesh Parkar Sep 27 '13 at 5:19
scrapyrt not receiving response from scrapy crawler,"I am trying to run scrapy crawler using scrapyrt. I get following response in browser {""status"": ""error"", ""message"": """", ""code"": 500} response: 1 and this one in scrapyrt window I have tried to edit the path of log file but it throws Permission denied error. The crawler runs successfully (as it creates html file) but not receiving json response in curl. $curl = curl_init(); curl_setopt_array($curl, array( CURLOPT_PORT=>'9080', CURLOPT_URL => ""http://localhost/crawl.json?spider_name=dmoz&url=http://www.dmoz.org/Computers/Programming/Languages/Ada/"", CURLOPT_FOLLOWLOCATION => true, CURLOPT_MAXREDIRS => 10, CURLOPT_USERAGENT => $_SERVER['HTTP_USER_AGENT'], CURLOPT_AUTOREFERER => true, CURLOPT_CONNECTTIMEOUT => 120, CURLOPT_TIMEOUT => 120, CURLOPT_POST => false )); $response = curl_exec($curl); $err = curl_error($curl); curl_close($curl); if ($err) { echo ""cURL Error #:"" . $err; } else { echo ""response: "".$response; } If the same crawler is executed from scapy cmd scrapy crawl dmoz -a url=""http://www.dmoz.org/Computers/Programming/Languages/Ada/"" the output is {'description': u'ACM Special Interest Group on Ada: information on SIGAda organization and pointers to current information and resources for the Ada programming language.', 'name': u'SIGAda', 'url': u'http://www.sigada.org/'}",0,,1 Answer 1,"Solved the issue: Updated ""C:\Python27\Lib\site-packages\scrapyrt\log.py"" file with following. Replaced filename = settings.get('LOG_FILE') with this filename = ""C:\\wamp64\\www\\dirbot-master\\logs\\dmoz\\log.log"" dirbot-master is scrapy project. Now I am receiving the response in browser.",1,,,,
Scrapy crawler in Cron job,"I want to execute my scrapy crawler from cron job . i create bash file getdata.sh where scrapy project is located with it's spiders #!/bin/bash cd /myfolder/crawlers/ scrapy crawl my_spider_name My crontab looks like this , I want to execute it in every 5 minute */5 * * * * sh /myfolder/crawlers/getdata.sh but it don't works , whats wrong , where is my error ? when I execute my bash file from terminal sh /myfolder/crawlers/getdata.sh it works fine",19,"is the sh ""prefix"" in */5 * * * * sh /myfolder/crawlers/getdata.sh necessary to execute shell scripts from crontab???",7 Answers 7,I solved this problem including PATH into bash file #!/bin/bash cd /myfolder/crawlers/ PATH=$PATH:/usr/local/bin export PATH scrapy crawl my_spider_name,25,+1 Had the same problem and simply couldn't figure it out. You should mark your question as the accepted answer. :) – Xethron Sep 21 '13 at 9:47,"Adding the following lines in crontab -e runs my scrapy crawl at 5AM every day. This is a slightly modified version of crocs' answer PATH=/usr/bin * 5 * * * cd project_folder/project_name/ && scrapy crawl spider_name Without setting $PATH, cron would give me an error ""command not found: scrapy"". I guess this is because /usr/bin is where scripts to run programs are stored in Ubuntu. Note that the complete path for my scrapy project is /home/user/project_folder/project_name. I ran the env command in cron and noticed that the working directory is /home/user. Hence I skipped /home/user in my crontab above The cron log can be helpful while debugging grep CRON /var/log/syslog|Another option is to forget using a shell script and chain the two commands together directly in the cronjob. Just make sure the PATH variable is set before the first scrapy cronjob in the crontab list. Run: crontab -e to edit and have a look. I have several scrapy crawlers which run at various times. Some every 5 mins, others twice a day. PATH=/usr/local/bin */5 * * * * user cd /myfolder/crawlers/ && scrapy crawl my_spider_name_1 * 1,13 * * * user cd /myfolder/crawlers/ && scrapy crawl my_spider_name_2 All jobs located after the PATH variable will find scrapy. Here the first one will run every 5 mins and the 2nd twice a day at 1am and 1pm. I found this easier to manage. If you have other binaries to run then you may need to add their locations to the path.|For anyone who used pip3 (or similar) to install scrapy, here is a simple inline solution: */10 * * * * cd ~/project/path && ~/.local/bin/scrapy crawl something >> ~/crawl.log 2>&1 Replace: */10 * * * * with your cron pattern ~/project/path with the path to your scrapy project (where your scrapy.cfg is) something with the spider name (use scrapy list in your project to find out) ~/crawl.log with your log file position (in case you want to have logging)|Check where scrapy is installed using ""which scrapy"" command. In my case, scrapy is installed in /usr/local/bin. Open crontab for editing using crontab -e. PATH=$PATH:/usr/local/bin export PATH */5 * * * * cd /myfolder/path && scrapy crawl spider_name It should work. Scrapy runs every 5 minutes.|does your shell script have execute permission? e.g. can you do /myfolder/crawlers/getdata.sh without the sh? if you can then you can drop the sh in the line in cron|in my case scrapy is in .local/bin/scrapy give the proper path of scraper and name it worK perfect 0 0 * * * cd /home/user/scraper/Folder_of_scriper/ && /home/user/.local/bin/scrapy crawl ""name"" >> /home/user/scrapy.log 2>&1 /home/user/scrapy.log it use to save the output and error in scrapy.log for check it program work or not thank you.","9,3,2,1,0,0",",,where does the path ~/.local/bin/scrapy come from or what is the significance of it? – oldboy Jul 2 '18 at 0:57,,No it writes that permissions is denied – beka Jun 21 '13 at 12:26,"
my scrapy crawler doesnt return results from amazon.com,"Folks I have been coding Scrapy based web crawlers for a couple of weeks. They seem to be working as expected. I have become a scrapy fan. But in the last couple of days my latest scrapy crawler refuses to crawl the Amazon site. I dont get any results back. Neither do I get any error codes. I have even tried the scrapy shell. It just doesnt return any results. I suspect the problem is in the xpath or the css expression, but I'm not able to figure it out. Any help will be gladly appreciated. Here's what my spider looks like My code prints the xxxxx and nothing after import scrapy from scrapy.spiders import CrawlSpider, Rule from scrapy.linkextractors import LinkExtractor from amazon.items import LowesItem from amazon.items import SwatchcolorItem class SattySpider(scrapy.Spider): name = ""faucets"" allowed_domains = [""amazon.com""] start_urls = [ ""https://www.amazon.com/s?ie=UTF8&page=1&rh=n%3A228013%2Ck%3Abathroom%20faucets"" ] rules = ( Rule(LinkExtractor(allow='amazon\.com/[A-Z][a-zA-Z_/]+$'), 'parse_category', follow=True, ), ) def parse(self, response): print 'xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx' ##### # I even tried xpath #for sel in response.xpath('.//li[@class=""s-result-item celwidget s-hidden-sponsored-item""]'): # prodDesc= sel.xpath('.//div[@class=""s-item-container""]//div[@class=""a-row a-spacing-none""]//a[@title]').extract() ##### for sel in response.css(""li.s-result-item.celwidget.s-hidden-sponsored-item > div.s-item-container > div > div > a::attr('href')""): #for sel in response.xpath('.//li[@class=""s-result-item celwidget s-hidden-sponsored-item""]'): prodDesc= sel.xpath('.//div[@class=""s-item-container""]//div[@class=""a-row a-spacing-none""]//a[@title]').extract() print prodDesc produrls = sel.xpath('.//@data-producturl').extract() urls = sel.xpath('.//@data-productimg').extract() #prod_url_det = response.urljoin(produrl.extract()) lowi= LowesItem() lowi['swatcharray'] = {} for idx,swatch in enumerate(sel.xpath('.//div[@class=""product-container js-product-container""]//a//div[@class=""pvs pvs-options-height v-spacing-small""]//ul/li')): swatchcolor = swatch.xpath('.//img//@alt').extract() lowi['swatcharray'][idx] =swatchcolor #yield lowi #url_prod_det = response.urljoin(produrl) for idx1,url in enumerate(urls): url_prod_det = response.urljoin(produrls[idx1]) yield scrapy.Request(url_prod_det, meta={'lowes': LowesItem(prod=prod[idx1], swatcharray=lowi['swatcharray'], file_urls=['http:' + url])}, callback=self.parse_productdetail) for next in response.css(""div.grid-parent.v-spacing-extra-large > nav > ul > li.page-next > a::attr('href')""): url_next = response.urljoin(next.extract()) print "" url_next : "" + url_next yield scrapy.Request(url_next, callback=self.parse) def parse_productdetail(self, response): print 'Testing....' # for model in response.xpath('//div[@class=""pd-numbers grid-50 tablet-grid-100""]//p[@class=""secondary-text small-type""]').re('<strong> Model # </strong>'): for model in response.xpath('//div[@class=""pd-numbers grid-50 tablet-grid-100""]//p[@class=""secondary-text small-type""]'): #print model.extract() modelname = model.xpath('./text()').extract() #print modelname #yield lowesItem lowesItem = response.meta['lowes'] lowesItem['model']=modelname[1] lowesItem['category']='default' lowesItem['subcategory']='default' lowesItem['vendor']='Lowes' for namevals in response.xpath('//div[@id=""collapseSpecs""]//div[@class=""panel-body""]//div[@class=""grid-100 grid-parent""]//div[@class=""grid-50""]//table[@class=""table full-width no-borders""]//tbody//tr'): #print namevals name = namevals.xpath('.//th/text()').extract() val = namevals.xpath('.//td//span/text()').extract() if 'Faucet Type' in name: lowesItem['faucettype']=val[0] elif 'Number of Faucet Handles' in name: lowesItem['numofhandles']=val[0] elif 'ADA Compliant' in name: lowesItem['ada']=val[0] elif 'Built-In Water Filter' in name: lowesItem['builtinwaterfilter']=val[0] elif 'Mounting Location' in name: lowesItem['mountingloc']=val[0] elif 'Color/Finish Family' in name: lowesItem['color']=val[0] elif 'Manufacturer Color/Finish' in name: lowesItem['manufacturercolor']=val[0] elif 'Collection Name' in name: lowesItem['collection']=val[0] elif 'Soap or Lotion Dispenser' in name: lowesItem['soapdispenser']=val[0] elif 'Spout Height (Inches)' in name: lowesItem['spoutheight']=val[0] elif 'Max Flow Rate' in name: lowesItem['maxflowrate']=val[0] yield lowesItem",1,"dont scrape amazon, use the API",0,,,,,,
Scrapy restart crawler when crawling finished,When my Scrapy crawler has finished I would like to automatically start the same crawler again. Could this be done with a Scrapy function or do I have to use a Cronjob e.g. crontab?,1,So it isn't possible to check if the crawler has finished and start the batch again? Disabling the dupe filter would work but how to re-request the same page?,1 Answer 1,,,,Just a normal Scrapy run can't do this unless you disable the dupe filter and upon downloading a page re-request the same page. It's a bit of a hacky solution but technically it would work. Cronjob or a Bash script that runs in a loop would do the trick.,1,So it isn't possible to check if the crawler has finished and start the batch again? Disabling the dupe filter would work but how to re-request the same page? – Jesper1 Oct 23 '13 at 17:29
How to get Python Scrapy Crawler details?,I am using the Python Scrapy tool to extract data from websites. I'm firing Scrapy from my php code using proc_open(). Now I need to maintain a Dashboard kind of thing. Is there a way in Scrapy to get Crawler details like: Time taken by Crawler to run. Start and Stop Time of crawler. Crawler Status (active or stopped). List of Crawlers running simultaneously.,3,You can write your own extension to store the any data you want to display in your dashboard. Then read in your app without interacting directly with scrapy. Do you need a more detailed answer?,1 Answer 1,"Your problem can be solved by using an extension. For example: from datetime import datetime from scrapy import signals from twisted.internet.task import LoopingCall class SpiderDetails(object): """"""Extension for collect spider information like start/stop time."""""" update_interval = 5 # in seconds def __init__(self, crawler): # keep a reference to the crawler in case is needed to access to more information self.crawler = crawler # keep track of polling calls per spider self.pollers = {} @classmethod def from_crawler(cls, crawler): instance = cls(crawler) crawler.signals.connect(instance.spider_opened, signal=signals.spider_opened) crawler.signals.connect(instance.spider_closed, signal=signals.spider_closed) return instance def spider_opened(self, spider): now = datetime.utcnow() # store curent timestamp in db as 'start time' for this spider # TODO: complete db calls # start activity poller poller = self.pollers[spider.name] = LoopingCall(self.spider_update, spider) poller.start(self.update_interval) def spider_closed(self, spider, reason): # store curent timestamp in db as 'end time' for this spider # TODO: complete db calls # remove and stop activity poller poller = self.pollers.pop(spider.name) poller.stop() def spider_update(self, spider): now = datetime.utcnow() # update 'last update time' for this spider # TODO: complete db calls pass Time taken by Crawler to run: that is end time - start time. You can calculate it when reading from db or storing as well with the end time. Start and Stop Time of crawler: that is stored in spider_opened and spider_closed methods. Crawler Status (Active or Stopped): your crawler is active if now - last update time is close to 5 seconds. Otherwise, if the last update was a long time ago (30 secs, 5 minutes or more), then your spider has either stopped abnormally or hanged up. If the spider record has an end time then the crawler has finished correctly. List of Crawlers running simultaneously: your frontend can query for the records with an empty end time. Those spiders will be either running or dead (in case the last update time was a long time ago). Take in consideration that the spider_closed signal will not be called in case the process finish abruptly. You will need to have a cron job to cleanup and/or update the dead records. Don't forget to add the extension to your settings.py file, like: EXTENSIONS = { # SpiderDetails class is in the file mybot/extensions.py 'mybot.extensions.SpiderDetails': 1000, }",4,@Rho.. Thanks for the detailed info on developing an Extension.. I will follow the info and let you know on my progress.. Thanks.. – kishan Oct 11 '13 at 7:00,,,
Scrapy Crawler in python cannot follow links?,"I wrote a crawler in python using the scrapy tool of python. The following is the python code: from scrapy.contrib.spiders import CrawlSpider, Rule from scrapy.contrib.linkextractors.sgml import SgmlLinkExtractor from scrapy.selector import HtmlXPathSelector #from scrapy.item import Item from a11ypi.items import AYpiItem class AYpiSpider(CrawlSpider): name = ""AYpi"" allowed_domains = [""a11y.in""] start_urls = [""http://a11y.in/a11ypi/idea/firesafety.html""] rules =( Rule(SgmlLinkExtractor(allow = ()) ,callback = 'parse_item') ) def parse_item(self,response): #filename = response.url.split(""/"")[-1] #open(filename,'wb').write(response.body) #testing codes ^ (the above) hxs = HtmlXPathSelector(response) item = AYpiItem() item[""foruri""] = hxs.select(""//@foruri"").extract() item[""thisurl""] = response.url item[""thisid""] = hxs.select(""//@foruri/../@id"").extract() item[""rec""] = hxs.select(""//@foruri/../@rec"").extract() return item But, instead of following the links the error thrown is: Traceback (most recent call last): File ""/usr/lib/python2.6/site-packages/Scrapy-0.12.0.2538-py2.6.egg/scrapy/cmdline.py"", line 131, in execute _run_print_help(parser, _run_command, cmd, args, opts) File ""/usr/lib/python2.6/site-packages/Scrapy-0.12.0.2538-py2.6.egg/scrapy/cmdline.py"", line 97, in _run_print_help func(*a, **kw) File ""/usr/lib/python2.6/site-packages/Scrapy-0.12.0.2538-py2.6.egg/scrapy/cmdline.py"", line 138, in _run_command cmd.run(args, opts) File ""/usr/lib/python2.6/site-packages/Scrapy-0.12.0.2538-py2.6.egg/scrapy/commands/crawl.py"", line 45, in run q.append_spider_name(name, **opts.spargs) --- <exception caught here> --- File ""/usr/lib/python2.6/site-packages/Scrapy-0.12.0.2538-py2.6.egg/scrapy/queue.py"", line 89, in append_spider_name spider = self._spiders.create(name, **spider_kwargs) File ""/usr/lib/python2.6/site-packages/Scrapy-0.12.0.2538-py2.6.egg/scrapy/spidermanager.py"", line 36, in create return self._spiders[spider_name](**spider_kwargs) File ""/usr/lib/python2.6/site-packages/Scrapy-0.12.0.2538-py2.6.egg/scrapy/contrib/spiders/crawl.py"", line 38, in __init__ self._compile_rules() File ""/usr/lib/python2.6/site-packages/Scrapy-0.12.0.2538-py2.6.egg/scrapy/contrib/spiders/crawl.py"", line 82, in _compile_rules self._rules = [copy.copy(r) for r in self.rules] exceptions.TypeError: 'Rule' object is not iterable Can someone please explain to me what's going on? Since this is the stuff mentioned in the documentation and I leave the allow field blank, that itself should make follow True by default. So why the error? What kind of optimisations can I make with my crawler to make it fast?",7,,1 Answer 1,,,,"From what I see, it looks like your rule is not an iterable. It looks like you were trying to make rules a tuple, you should read up on tuples in the python documentation. To fix your problem, change this line: rules =( Rule(SgmlLinkExtractor(allow = ()) ,callback = 'parse_item') ) To: rules =(Rule(SgmlLinkExtractor(allow = ()) ,callback = 'parse_item'),) Notice the comma at the end?",33,
Running a Scrapy Crawler,"I am very new in Python and Scrapy and I have written a crawler in PyCharm as follow: import scrapy from scrapy.spiders import Spider from scrapy.http import Request import re class TutsplusItem(scrapy.Item): title = scrapy.Field() class MySpider(Spider): name = ""tutsplus"" allowed_domains = [""bbc.com""] start_urls = [""http://www.bbc.com/""] def parse(self, response): links = response.xpath('//a/@href').extract() # We stored already crawled links in this list crawledLinks = [] for link in links: # If it is a proper link and is not checked yet, yield it to the Spider #if linkPattern.match(link) and not link in crawledLinks: if not link in crawledLinks: link = ""http://www.bbc.com"" + link crawledLinks.append(link) yield Request(link, self.parse) titles = response.xpath('//a[contains(@class, ""media__link"")]/text()').extract() for title in titles: item = TutsplusItem() item[""title""] = title print(""Title is : %s"" %title) yield item However, when I run above codes, nothing prints on the screen! What is wrong in my code?",0,,3 Answers 3,,,,"You would typically start scrapy using scrapy crawl, which will hook everything up for you and start the crawling. It also looks like your code is not properly indented (only one line inside parse when they all should be).|Put the code in a text file, name it to something like your_spider.py and run the spider using the runspider command: scrapy runspider your_spider.py|To run a spider from within Pycharm you need to configure ""Run/Debug configuration"" properly. Running your_spider.py as a standalone script wouldn't result in anything. As mentioned by @stranac scrapy crawl is the way to go. With scrapy being a binary and crawl an argument of your binary. Configure Run/Debug In the main menu go to : Run > Run Configurations... Find the appropriate scrapy binary within your virtualenv and set its absolute path as Script. This should look like something like this: /home/username/.virtualenvs/your_virtualenv_name/bin/scrapy In Scrapy parameters set up the parameters the binary scrapy will execute. In your case, you wan to start your spider. this is how this should look like: crawl your_spider_name e.g. crawl tutsplus Make sure that the Python intrepreter is the one where you setup Scrapy and other packages needed for your project. Make sure that the working directory is the directory containing settings.pywhich is also generated by Scrapy. From now on you should be able to Run and Debug your spiders from within Pycharm.","0,0,0",",,"
Scrapy crawler getting partial data,"I'm using Scrapy for crawling some website. I need to get data every hour so I created a crontab to launch my crawlers. I made a python script for every crawler and another script that launch every ""subscript"". So I have a ""master"" script that it's like ""os.system(""cd /home/.../directory1 ; python directory1Launch.py"")"" and some ""slave"" script that are like ""os.system(""scrapy crawl directory 1 -a start_url \""urls\"" -o data.json"")"" for a certain number of crawlers. That was working fine. Then I had to add some function in pipeline. Now a couple of crawler (that are working on the same site) crawl just a fraction of the data (2 items instead of 7 items). The fact is that if I launch the ""master"" script manually everithing works just fine. And the other crawler are working just like before. Maybe it's a time problem (parser taking too much time?) but it would happen also when manually launched... Any idea?",0,Is each script making more than one query?,0,,,,,,
Scrapy Crawler not following links,"I am writing a Scrapy crawler to scrape information from a property website, https://www.iproperty.com.sg/sale/?page=1, https://www.iproperty.com.sg/sale/?page=2 etc.. The idea is that for each row, obtain information from that row and make a request to a link on that row for further information. Once all rows on that page have been processed, move on to the next page and repeat: import scrapy from scrapy.linkextractors import LinkExtractor from scrapy.spiders import CrawlSpider, Rule from property.items import PropertyItem class IpropCrawlerSpider(CrawlSpider): name = 'iprop_crawler' allowed_domains = ['www.iproperty.com.sg'] start_urls = [""https://www.iproperty.com.sg/sale/?page=1""] rules = ( Rule(LinkExtractor(allow=r'sale\/\?page=[1-9]'), callback='parse_item', follow=True), ) def parse_item(self, response): prop_list_xpath = '//h3[@class=""cgiArp""]' for prop in response.xpath(prop_list_xpath): item = PropertyItem() item['name'] = prop.xpath('./a/text()').extract_first() deep_uri = prop.xpath('./a/@href').extract_first() deep_url = 'https://www.iproperty.com.sg' + deep_uri request = scrapy.Request(deep_url, callback=self.parse_per_prop) request.meta['item'] = item yield request def parse_per_prop(self, response): item = response.meta['item'] item['price'] = response\ .xpath('//div[@class=""property-price duzTnm""]/text()')\ .extract_first() item['address'] = response\ .xpath('//span[@class=""property-address sale-default""]/text()')\ .extract_first() item['property_type'] = response\ .xpath('//div[@class=""property-attr-propertyType cXGbLS""]' \ + '/div[2]/text()')\ .extract_first() yield item Running this crawler results in no data scraped: 2018-11-09 01:53:58 [scrapy.utils.log] INFO: Scrapy 1.5.1 started (bot: property) 2018-11-09 01:53:58 [scrapy.utils.log] INFO: Versions: lxml 3.7.2.0, libxml2 2.9.4, cssselect 1.0.0, parsel 1.5.0, w3lib 1.17.0, Twisted 17.1.0, Python 3.6.1 |Anaconda custom (64-bit)| (default, Mar 22 2017, 19:54:23) - [GCC 4.4.7 20120313 (Red Hat 4.4.7-1)], pyOpenSSL 16.2.0 (OpenSSL 1.0.2p 14 Aug 2018), cryptography 1.7.1, Platform Linux-4.18.16-arch1-1-ARCH-x86_64-with-arch 2018-11-09 01:53:58 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'property', 'DOWNLOAD_DELAY': 1, 'NEWSPIDER_MODULE': 'property.spiders', 'ROBOTSTXT_OBEY': True, 'SPIDER_MODULES': ['property.spiders']} 2018-11-09 01:53:58 [scrapy.middleware] INFO: Enabled extensions: ['scrapy.extensions.corestats.CoreStats', 'scrapy.extensions.telnet.TelnetConsole', 'scrapy.extensions.memusage.MemoryUsage', 'scrapy.extensions.logstats.LogStats'] 2018-11-09 01:53:58 [scrapy.middleware] INFO: Enabled downloader middlewares: ['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware', 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware', 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware', 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware', 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware', 'scrapy.downloadermiddlewares.retry.RetryMiddleware', 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware', 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware', 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware', 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware', 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware', 'scrapy.downloadermiddlewares.stats.DownloaderStats'] 2018-11-09 01:53:58 [scrapy.middleware] INFO: Enabled spider middlewares: ['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware', 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware', 'scrapy.spidermiddlewares.referer.RefererMiddleware', 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware', 'scrapy.spidermiddlewares.depth.DepthMiddleware'] 2018-11-09 01:53:58 [scrapy.middleware] INFO: Enabled item pipelines: [] 2018-11-09 01:53:58 [scrapy.core.engine] INFO: Spider opened 2018-11-09 01:53:58 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min) 2018-11-09 01:53:58 [scrapy.extensions.telnet] DEBUG: Telnet console listening on 127.0.0.1:6024 2018-11-09 01:53:58 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.iproperty.com.sg/robots.txt> (referer: None) 2018-11-09 01:54:01 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.iproperty.com.sg/sale/?page=1> (referer: None) 2018-11-09 01:54:01 [scrapy.core.engine] INFO: Closing spider (finished) 2018-11-09 01:54:01 [scrapy.statscollectors] INFO: Dumping Scrapy stats: {'downloader/request_bytes': 460, 'downloader/request_count': 2, 'downloader/request_method_count/GET': 2, 'downloader/response_bytes': 154841, 'downloader/response_count': 2, 'downloader/response_status_count/200': 2, 'finish_reason': 'finished', 'finish_time': datetime.datetime(2018, 11, 8, 17, 54, 1, 224281), 'log_count/DEBUG': 3, 'log_count/INFO': 7, 'memusage/max': 47136768, 'memusage/startup': 47136768, 'response_received_count': 2, 'scheduler/dequeued': 1, 'scheduler/dequeued/memory': 1, 'scheduler/enqueued': 1, 'scheduler/enqueued/memory': 1, 'start_time': datetime.datetime(2018, 11, 8, 17, 53, 58, 676635)} 2018-11-09 01:54:01 [scrapy.core.engine] INFO: Spider closed (finished) If I change parse_item to parse_start_url only the first page is scraped but the following links are not followed: 2018-11-09 02:11:42 [scrapy.statscollectors] INFO: Dumping Scrapy stats: {'downloader/request_bytes': 6195, 'downloader/request_count': 20, 'downloader/request_method_count/GET': 20, 'downloader/response_bytes': 2433163, 'downloader/response_count': 20, 'downloader/response_status_count/200': 20, 'finish_reason': 'shutdown', 'finish_time': datetime.datetime(2018, 11, 8, 18, 11, 42, 430358), 'item_scraped_count': 18, 'log_count/DEBUG': 39, 'log_count/INFO': 8, 'memusage/max': 47132672, 'memusage/startup': 47132672, 'request_depth_max': 1, 'response_received_count': 20, 'scheduler/dequeued': 19, 'scheduler/dequeued/memory': 19, 'scheduler/enqueued': 21, 'scheduler/enqueued/memory': 21, 'start_time': datetime.datetime(2018, 11, 8, 18, 11, 18, 416991)} 2018-11-09 02:11:42 [scrapy.core.engine] INFO: Spider closed (shutdown) I would like to seek enlightenment on this issue as to why I am unable to follow the link to the next pages.",2,"Unfortunately passing a string as an argument does not seem to change the result. Also, should it not be 'sale\/\?page=[1-9]\d*'?",2 Answers 2,So I discovered that there was a problem with the rule itself and had to use an xpath selector instead.,-1,,"Judging by the Scrapy documentation, it looks like your passing a reference to your parse_item method to the callback argument of the rule. However, according to the docs, this callback operates on the extracted links. That's not what you want because your function requires a Scrapy Response to run. So, what you should do is use the process_request argument. On a related note, I changed your regex because the way you have it now it'll only work for pages 1 to 9 rules = ( Rule(LinkExtractor(allow = r'sale\/\?page=[1-9]\d*'), process_request = 'parse_item', follow = True), ) As an aside, you probably shouldn't return a Request object back to Scrapy and instead should use scrapy.Item and ItemLoader to store your data.",2,"Unfortunately passing a string as an argument does not seem to change the result. Also, should it not be 'sale\/\?page=[1-9]\d*'? – Kevin Tham Nov 9 '18 at 17:02"
Scrapy Crawler excel output,"I am new to python and scrapy, however I was trying to develop a crawler and scraper to extract list of products on an amazon page, the scraped info must have name, price and prime availability. Items are scraped however every item scraped is when outputted in a csv file is entirely in one single cell. All I want is to make each product and its corresponding details be outputted in each cell distinctly. The logic is: items= [] for products in response.xpath('//*[@id=""mainResults""]/ul'): item = amazonlist() item['Title'] = products.css('a>h2::text').extract() item['Price'] = products.css(' div > div > div > a > span.a-size-base.a-color-price.s-price.a-text-bold::text').extract() item['Prime'] = products.css(' div > div > div > i::attr(aria-label)').extract() items.append(item) return items Can you guide me with this ?",0,"But that would still require human intervention, all I want is to directly output the contents organized in excel sheet.",2 Answers 2,,,,"I've been playing with some web scrapping my self recently. The way I've been pulling things from a web page has been by using lxml to get the html and then I store that into a text file and then sort through it from there. Hope I helped.|Since I can't run your example and have only part of your code, two ideas. Both may or may not cause your data being conglomerated in one cell: You are returning all items at once, rather than yielding them one by one (look up Python generators if you're unsure what this means). Try this instead: items= [] for products in response.xpath('//*[@id=""mainResults""]/ul'): item = amazonlist() item['Title'] = products.css('a>h2::text').extract() item['Price'] = products.css(' div > div > div > a > span.a-size-base.a-color-price.s-price.a-text-bold::text').extract() item['Prime'] = products.css(' div > div > div > i::attr(aria-label)').extract() yield item The extract method returns a list of results, one for every match. If every products instance in the loop contains multiple products in itself, they all match. You would have to break the query further down to loop through each individual product. You can use pdb or a print statement to check if, for example, item['Title'] contains a list of strings rather than one. Hope this helps!","0,0","But that would still require human intervention, all I want is to directly output the contents organized in excel sheet. – raj shastri Jan 22 '17 at 20:59,I have even tried this code, I still get the very same output. – raj shastri Jan 26 '17 at 6:57"
"Scrapy crawler, removing comma from the string","def parse_item(self, response): for jobs in response.xpath('//div[@itemtype=""http://schema.org/JobPosting""]'): item = IndeedCoUkItem() item[""jobtitle""] = jobs.xpath('*[@class=""jobtitle""]/a//text()').extract() yield item item saved to a CSV file as, jobtitle ""Senior ,Embedded, ,Software, Engineer"" Hi, The above is a snippet from my scrapy crawler code. I would like to have the output be comma and white space free. That is from ""Senior ,Embedded, ,Software, Engineer"" to this ""Senior Embedded Software Engineer"". I tried to use replace() like ..extract()[0].replace("","",""""), but it didn't work. Any help/ suggestion?",4,"GHajba, your solutions seems to work (no more comma) but I got whitespace remained there. Here is the actual output, [u'Senior ', u'Embedded', u' ', u'Software', u' Engineer'] [u'Embedded', u' C'] [u'Software', u' Engineer, Compiler']",2 Answers 2,"Did you try to print / log the list which gets into the item['jobtitle] field? If it is a list (well, it is a list) then the export to a CSV file converts this list to a comma separated entry. Try to look at the result and join the list to one: item[""jobtitle""] = ' '.join(jobs.xpath('*[@class=""jobtitle""]/a//text()').extract()) If the items contain extra white-spaces but not all of them, you can use map and strip on the elements: item[""jobtitle""] = ' '.join(map(unicode.strip,jobs.xpath('*[@class=""jobtitle""]/a//text()').extract())) This walks throug all the elements and strips off the whitespaces at the beginning and at the end. Alternatively you could use normalize-space of XPath: item[""jobtitle""] = ' '.join(jobs.xpath('normalize-space(*[@class=""jobtitle""]/a//text())').extract())",3,"GHajba, your solutions seems to work (no more comma) but I got whitespace remained there. Here is the actual output, [u'Senior ', u'Embedded', u' ', u'Software', u' Engineer'] [u'Embedded', u' C'] [u'Software', u' Engineer, Compiler'] – ibrahimdanish Sep 14 '15 at 11:37","item[""jobtitle""] = (jobs.xpath('*[@class=""jobtitle""]/a//text()').extract()).replace(',', '')",-1,"There is no explanation of what this code does, or why. Please add clarification to add to the value that this answer brings. – rcbevans Sep 14 '15 at 14:49"
scrapy crawler caught exception reading instance data,"I am new to python and want to use scrapy to build a web crawler. I go through the tutorial in http://blog.siliconstraits.vn/building-web-crawler-scrapy/. Spider code likes following: from scrapy.spider import BaseSpider from scrapy.selector import HtmlXPathSelector from nettuts.items import NettutsItem from scrapy.http import Request class MySpider(BaseSpider): name = ""nettuts"" allowed_domains = [""net.tutsplus.com""] start_urls = [""http://net.tutsplus.com/""] def parse(self, response): hxs = HtmlXPathSelector(response) titles = hxs.select('//h1[@class=""post_title""]/a/text()').extract() for title in titles: item = NettutsItem() item[""title""] = title yield item When launch the spider with command line: scrapy crawl nettus, it has following error: [boto] DEBUG: Retrieving credentials from metadata server. 2015-07-05 18:27:17 [boto] ERROR: Caught exception reading instance data Traceback (most recent call last): File ""/anaconda/lib/python2.7/site-packages/boto/utils.py"", line 210, in retry_url r = opener.open(req, timeout=timeout) File ""/anaconda/lib/python2.7/urllib2.py"", line 431, in open response = self._open(req, data) File ""/anaconda/lib/python2.7/urllib2.py"", line 449, in _open '_open', req) File ""/anaconda/lib/python2.7/urllib2.py"", line 409, in _call_chain result = func(*args) File ""/anaconda/lib/python2.7/urllib2.py"", line 1227, in http_open return self.do_open(httplib.HTTPConnection, req) File ""/anaconda/lib/python2.7/urllib2.py"", line 1197, in do_open raise URLError(err) URLError: <urlopen error [Errno 65] No route to host> 2015-07-05 18:27:17 [boto] ERROR: Unable to read instance data, giving up really do not know what's wrong. Hope somebody could help",10,Is that the full traceback (I'm guessing it's not)/,2 Answers 2,"in the settings.py file: add following code settings: DOWNLOAD_HANDLERS = {'s3': None,}",28,Where is this documented? – gusridd Oct 15 '15 at 3:34,The important information is: URLError: <urlopen error [Errno 65] No route to host> That is trying to tell you that your computer doesn't know how to communicate with the site you're trying to scrape. Are you able to access the site normally (i.e. in a web-browser) from the machine you're trying to run this python on?,0,"Well, I'm not sure how to help you further. All I can say is that ""No route to host"" is a networking problem telling you that the OS doesn't know how to send packets to the IP address it needs to send packets to. – CrazyCasta Jul 5 '15 at 16:54"
Difference between BeautifulSoup and Scrapy crawler?,I want to make a website that shows the comparison between amazon and e-bay product price. Which of these will work better and why? I am somewhat familiar with BeautifulSoup but not so much with Scrapy crawler.,132,Why would you use crawlers when both those sites have a great API? aws.amazon.com/python developer.ebay.com/common/api,8 Answers 8,"Scrapy is a Web-spider or web scraper framework, You give Scrapy a root URL to start crawling, then you can specify constraints on how many (number of) URLs you want to crawl and fetch,etc. It is a complete framework for web-scraping or crawling. While BeautifulSoup is a parsing library which also does a pretty good job of fetching contents from URL and allows you to parse certain parts of them without any hassle. It only fetches the contents of the URL that you give and then stops. It does not crawl unless you manually put it inside an infinite loop with certain criteria. In simple words, with Beautiful Soup you can build something similar to Scrapy. Beautiful Soup is a library while Scrapy is a complete framework. Source",228,"which is faster , I mean i am using BeautifulSoup and it takes around 10sec to scrap data ? does scrapy faster than beautifulsoup ? – shuboy2014 Jun 22 '16 at 9:38","I think both are good... im doing a project right now that use both. First i scrap all the pages using scrapy and save that on a mongodb collection using their pipelines, also downloading the images that exists on the page. After that i use BeautifulSoup4 to make a pos-processing where i must change attributes values and get some special tags. If you don't know which pages products you want, a good tool will be scrapy since you can use their crawlers to run all amazon/ebay website looking for the products without making a explicit for loop. Take a look at the scrapy documentation, it's very simple to use.|Both are using to parse data. Scrapy: Scrapy is a fast high-level web crawling and web scraping framework, used to crawl websites and extract structured data from their pages. But it has some limitations when data comes from java script or loading dynamicaly, we can over come it by using packages like splash, selenium etc. BeautifulSoup: Beautiful Soup is a Python library for pulling data out of HTML and XML files. we can use this package for getting data from java script or dynamically loading pages. Scrapy with BeautifulSoup is one of the best combo we can work with for scraping static and dynamic contents|The way I do it is to use the eBay/Amazon API's rather than scrapy, and then parse the results using BeautifulSoup. The APIs gives you an official way of getting the same data that you would have got from scrapy crawler, with no need to worry about hiding your identity, mess about with proxies,etc.|Scrapy It is a web scraping framework which comes with tons of goodies which make scraping from easier so that we can focus on crawling logic only. Some of my favourite things scrapy takes care for us are below. Feed exports: It basically allows us to save data in various formats like CSV,JSON,jsonlines and XML. Asynchronous scraping: Scrapy uses twisted framework which gives us power to visit multiple urls at once where each request is processed in non blocking way(Basically we don't have to wait for a request to finish before sending another request). Selectors: This is where we can compare scrapy with beautiful soup. Selectors are what allow us to select particular data from the webpage like heading, certain div with a class name etc.). Scrapy uses lxml for parsing which is extremely fast than beautiful soup. Setting proxy,user agent ,headers etc: scrapy allows us to set and rotate proxy,and other headers dynamically. Item Pipelines: Pipelines enable us to process data after extraction. For example we can configure pipeline to push data to your mysql server. Cookies: scrapy automatically handles cookies for us. etc. TLDR: scrapy is a framework that provides everything that one might need to build large scale crawls. It provides various features that hide complexity of crawling the webs. one can simply start writing web crawlers without worrying about the setup burden. Beautiful soup Beautiful Soup is a Python package for parsing HTML and XML documents. So with Beautiful soup you can parse a webpage that has been already downloaded. BS4 is very popular and old. Unlike scrapy,You cannot use beautiful soup only to make crawlers. You will need other libraries like requests,urllib etc to make crawlers with bs4. Again, this means you would need to manage the list of urls being crawled,to be crawled, handle cookies , manage proxy, handle errors, create your own functions to push data to CSV,JSON,XML etc. If you want to speed up than you will have to use other libraries like multiprocessing. To sum up. Scrapy is a rich framework that you can use to start writing crawlers without any hassale. Beautiful soup is a library that you can use to parse a webpage. It cannot be used alone to scrape web. You should definitely use scrapy for your amazon and e-bay product price comparison website. You could build a database of urls and run the crawler every day(cron jobs,Celery for scheduling crawls) and update the price on your database.This way your website will always pull from the database and crawler and database will act as individual components.|BeautifulSoup is a library that lets you extract information from a web page. Scrapy on the other hand is a framework, which does the above thing and many more things you probably need in your scraping project like pipelines for saving data. You can check this blog to get started with Scrapy https://www.inkoop.io/blog/web-scraping-using-python-and-scrapy/|Using scrapy you can save tons of code and start with structured programming, If you dont like any of the scapy's pre-written methods then BeautifulSoup can be used in the place of scrapy method. Big project takes both advantages.|The differences are many and selection of any tool/technology depends on individual needs. Few major differences are: BeautifulSoup is comparatively is easy to learn than Scrapy. The extensions, support, community is larger for Scrapy than for BeautifulSoup. Scrapy should be considered as a Spider while BeautifulSoup is a Parser.","18,4,2,2,1,0,0","So can i use Scrapy on web server because there are many dependencies of it like (Twisted, pywin32, pyOpenSSL ete..). (Sorry for this silly question, i am new to python) – Nishant Bhakta Oct 30 '13 at 16:00,,The question clearly asks for solutions where APIs are not available. – Rohanil Mar 16 '17 at 2:05,,,,"
