_question,_qv_content,_qv_vote,_qv_comment,total_answer_count,accepted_answer,at_vote,at_comment,suggestedanswers,sv_vote,sv_comment
Difference between BeautifulSoup and Scrapy crawler?,I want to make a website that shows the comparison between amazon and e-bay product price. Which of these will work better and why? I am somewhat familiar with BeautifulSoup but not so much with Scrapy crawler.,133,Why would you use crawlers when both those sites have a great API? aws.amazon.com/python developer.ebay.com/common/api,8 Answers 8,"Scrapy is a Web-spider or web scraper framework, You give Scrapy a root URL to start crawling, then you can specify constraints on how many (number of) URLs you want to crawl and fetch,etc. It is a complete framework for web-scraping or crawling. While BeautifulSoup is a parsing library which also does a pretty good job of fetching contents from URL and allows you to parse certain parts of them without any hassle. It only fetches the contents of the URL that you give and then stops. It does not crawl unless you manually put it inside an infinite loop with certain criteria. In simple words, with Beautiful Soup you can build something similar to Scrapy. Beautiful Soup is a library while Scrapy is a complete framework. Source",231,"which is faster , I mean i am using BeautifulSoup and it takes around 10sec to scrap data ? does scrapy faster than beautifulsoup ? – shuboy2014 Jun 22 '16 at 9:38","I think both are good... im doing a project right now that use both. First i scrap all the pages using scrapy and save that on a mongodb collection using their pipelines, also downloading the images that exists on the page. After that i use BeautifulSoup4 to make a pos-processing where i must change attributes values and get some special tags. If you don't know which pages products you want, a good tool will be scrapy since you can use their crawlers to run all amazon/ebay website looking for the products without making a explicit for loop. Take a look at the scrapy documentation, it's very simple to use.|Both are using to parse data. Scrapy: Scrapy is a fast high-level web crawling and web scraping framework, used to crawl websites and extract structured data from their pages. But it has some limitations when data comes from java script or loading dynamicaly, we can over come it by using packages like splash, selenium etc. BeautifulSoup: Beautiful Soup is a Python library for pulling data out of HTML and XML files. we can use this package for getting data from java script or dynamically loading pages. Scrapy with BeautifulSoup is one of the best combo we can work with for scraping static and dynamic contents|The way I do it is to use the eBay/Amazon API's rather than scrapy, and then parse the results using BeautifulSoup. The APIs gives you an official way of getting the same data that you would have got from scrapy crawler, with no need to worry about hiding your identity, mess about with proxies,etc.|Scrapy It is a web scraping framework which comes with tons of goodies which make scraping from easier so that we can focus on crawling logic only. Some of my favourite things scrapy takes care for us are below. Feed exports: It basically allows us to save data in various formats like CSV,JSON,jsonlines and XML. Asynchronous scraping: Scrapy uses twisted framework which gives us power to visit multiple urls at once where each request is processed in non blocking way(Basically we don't have to wait for a request to finish before sending another request). Selectors: This is where we can compare scrapy with beautiful soup. Selectors are what allow us to select particular data from the webpage like heading, certain div with a class name etc.). Scrapy uses lxml for parsing which is extremely fast than beautiful soup. Setting proxy,user agent ,headers etc: scrapy allows us to set and rotate proxy,and other headers dynamically. Item Pipelines: Pipelines enable us to process data after extraction. For example we can configure pipeline to push data to your mysql server. Cookies: scrapy automatically handles cookies for us. etc. TLDR: scrapy is a framework that provides everything that one might need to build large scale crawls. It provides various features that hide complexity of crawling the webs. one can simply start writing web crawlers without worrying about the setup burden. Beautiful soup Beautiful Soup is a Python package for parsing HTML and XML documents. So with Beautiful soup you can parse a webpage that has been already downloaded. BS4 is very popular and old. Unlike scrapy,You cannot use beautiful soup only to make crawlers. You will need other libraries like requests,urllib etc to make crawlers with bs4. Again, this means you would need to manage the list of urls being crawled,to be crawled, handle cookies , manage proxy, handle errors, create your own functions to push data to CSV,JSON,XML etc. If you want to speed up than you will have to use other libraries like multiprocessing. To sum up. Scrapy is a rich framework that you can use to start writing crawlers without any hassale. Beautiful soup is a library that you can use to parse a webpage. It cannot be used alone to scrape web. You should definitely use scrapy for your amazon and e-bay product price comparison website. You could build a database of urls and run the crawler every day(cron jobs,Celery for scheduling crawls) and update the price on your database.This way your website will always pull from the database and crawler and database will act as individual components.|BeautifulSoup is a library that lets you extract information from a web page. Scrapy on the other hand is a framework, which does the above thing and many more things you probably need in your scraping project like pipelines for saving data. You can check this blog to get started with Scrapy https://www.inkoop.io/blog/web-scraping-using-python-and-scrapy/|Using scrapy you can save tons of code and start with structured programming, If you dont like any of the scapy's pre-written methods then BeautifulSoup can be used in the place of scrapy method. Big project takes both advantages.|The differences are many and selection of any tool/technology depends on individual needs. Few major differences are: BeautifulSoup is comparatively is easy to learn than Scrapy. The extensions, support, community is larger for Scrapy than for BeautifulSoup. Scrapy should be considered as a Spider while BeautifulSoup is a Parser.","18,4,2,2,1,0,0","So can i use Scrapy on web server because there are many dependencies of it like (Twisted, pywin32, pyOpenSSL ete..). (Sorry for this silly question, i am new to python) – Nishant Bhakta Oct 30 '13 at 16:00,,The question clearly asks for solutions where APIs are not available. – Rohanil Mar 16 '17 at 2:05,,,,"
Scrapy crawler getting partial data,"I'm using Scrapy for crawling some website. I need to get data every hour so I created a crontab to launch my crawlers. I made a python script for every crawler and another script that launch every ""subscript"". So I have a ""master"" script that it's like ""os.system(""cd /home/.../directory1 ; python directory1Launch.py"")"" and some ""slave"" script that are like ""os.system(""scrapy crawl directory 1 -a start_url \""urls\"" -o data.json"")"" for a certain number of crawlers. That was working fine. Then I had to add some function in pipeline. Now a couple of crawler (that are working on the same site) crawl just a fraction of the data (2 items instead of 7 items). The fact is that if I launch the ""master"" script manually everithing works just fine. And the other crawler are working just like before. Maybe it's a time problem (parser taking too much time?) but it would happen also when manually launched... Any idea?",0,Is each script making more than one query?,0,,,,,,
Scrapy Crawler excel output,"I am new to python and scrapy, however I was trying to develop a crawler and scraper to extract list of products on an amazon page, the scraped info must have name, price and prime availability. Items are scraped however every item scraped is when outputted in a csv file is entirely in one single cell. All I want is to make each product and its corresponding details be outputted in each cell distinctly. The logic is: items= [] for products in response.xpath('//*[@id=""mainResults""]/ul'): item = amazonlist() item['Title'] = products.css('a>h2::text').extract() item['Price'] = products.css(' div > div > div > a > span.a-size-base.a-color-price.s-price.a-text-bold::text').extract() item['Prime'] = products.css(' div > div > div > i::attr(aria-label)').extract() items.append(item) return items Can you guide me with this ?",0,"But that would still require human intervention, all I want is to directly output the contents organized in excel sheet.",2 Answers 2,,,,"I've been playing with some web scrapping my self recently. The way I've been pulling things from a web page has been by using lxml to get the html and then I store that into a text file and then sort through it from there. Hope I helped.|Since I can't run your example and have only part of your code, two ideas. Both may or may not cause your data being conglomerated in one cell: You are returning all items at once, rather than yielding them one by one (look up Python generators if you're unsure what this means). Try this instead: items= [] for products in response.xpath('//*[@id=""mainResults""]/ul'): item = amazonlist() item['Title'] = products.css('a>h2::text').extract() item['Price'] = products.css(' div > div > div > a > span.a-size-base.a-color-price.s-price.a-text-bold::text').extract() item['Prime'] = products.css(' div > div > div > i::attr(aria-label)').extract() yield item The extract method returns a list of results, one for every match. If every products instance in the loop contains multiple products in itself, they all match. You would have to break the query further down to loop through each individual product. You can use pdb or a print statement to check if, for example, item['Title'] contains a list of strings rather than one. Hope this helps!","0,0","But that would still require human intervention, all I want is to directly output the contents organized in excel sheet. – raj shastri Jan 22 '17 at 20:59,I have even tried this code, I still get the very same output. – raj shastri Jan 26 '17 at 6:57"
"Scrapy crawler, removing comma from the string","def parse_item(self, response): for jobs in response.xpath('//div[@itemtype=""http://schema.org/JobPosting""]'): item = IndeedCoUkItem() item[""jobtitle""] = jobs.xpath('*[@class=""jobtitle""]/a//text()').extract() yield item item saved to a CSV file as, jobtitle ""Senior ,Embedded, ,Software, Engineer"" Hi, The above is a snippet from my scrapy crawler code. I would like to have the output be comma and white space free. That is from ""Senior ,Embedded, ,Software, Engineer"" to this ""Senior Embedded Software Engineer"". I tried to use replace() like ..extract()[0].replace("","",""""), but it didn't work. Any help/ suggestion?",4,"GHajba, your solutions seems to work (no more comma) but I got whitespace remained there. Here is the actual output, [u'Senior ', u'Embedded', u' ', u'Software', u' Engineer'] [u'Embedded', u' C'] [u'Software', u' Engineer, Compiler']",2 Answers 2,"Did you try to print / log the list which gets into the item['jobtitle] field? If it is a list (well, it is a list) then the export to a CSV file converts this list to a comma separated entry. Try to look at the result and join the list to one: item[""jobtitle""] = ' '.join(jobs.xpath('*[@class=""jobtitle""]/a//text()').extract()) If the items contain extra white-spaces but not all of them, you can use map and strip on the elements: item[""jobtitle""] = ' '.join(map(unicode.strip,jobs.xpath('*[@class=""jobtitle""]/a//text()').extract())) This walks throug all the elements and strips off the whitespaces at the beginning and at the end. Alternatively you could use normalize-space of XPath: item[""jobtitle""] = ' '.join(jobs.xpath('normalize-space(*[@class=""jobtitle""]/a//text())').extract())",3,"GHajba, your solutions seems to work (no more comma) but I got whitespace remained there. Here is the actual output, [u'Senior ', u'Embedded', u' ', u'Software', u' Engineer'] [u'Embedded', u' C'] [u'Software', u' Engineer, Compiler'] – ibrahimdanish Sep 14 '15 at 11:37","item[""jobtitle""] = (jobs.xpath('*[@class=""jobtitle""]/a//text()').extract()).replace(',', '')",-1,"There is no explanation of what this code does, or why. Please add clarification to add to the value that this answer brings. – rcbevans Sep 14 '15 at 14:49"
How to get Python Scrapy Crawler details?,I am using the Python Scrapy tool to extract data from websites. I'm firing Scrapy from my php code using proc_open(). Now I need to maintain a Dashboard kind of thing. Is there a way in Scrapy to get Crawler details like: Time taken by Crawler to run. Start and Stop Time of crawler. Crawler Status (active or stopped). List of Crawlers running simultaneously.,3,You can write your own extension to store the any data you want to display in your dashboard. Then read in your app without interacting directly with scrapy. Do you need a more detailed answer?,1 Answer 1,"Your problem can be solved by using an extension. For example: from datetime import datetime from scrapy import signals from twisted.internet.task import LoopingCall class SpiderDetails(object): """"""Extension for collect spider information like start/stop time."""""" update_interval = 5 # in seconds def __init__(self, crawler): # keep a reference to the crawler in case is needed to access to more information self.crawler = crawler # keep track of polling calls per spider self.pollers = {} @classmethod def from_crawler(cls, crawler): instance = cls(crawler) crawler.signals.connect(instance.spider_opened, signal=signals.spider_opened) crawler.signals.connect(instance.spider_closed, signal=signals.spider_closed) return instance def spider_opened(self, spider): now = datetime.utcnow() # store curent timestamp in db as 'start time' for this spider # TODO: complete db calls # start activity poller poller = self.pollers[spider.name] = LoopingCall(self.spider_update, spider) poller.start(self.update_interval) def spider_closed(self, spider, reason): # store curent timestamp in db as 'end time' for this spider # TODO: complete db calls # remove and stop activity poller poller = self.pollers.pop(spider.name) poller.stop() def spider_update(self, spider): now = datetime.utcnow() # update 'last update time' for this spider # TODO: complete db calls pass Time taken by Crawler to run: that is end time - start time. You can calculate it when reading from db or storing as well with the end time. Start and Stop Time of crawler: that is stored in spider_opened and spider_closed methods. Crawler Status (Active or Stopped): your crawler is active if now - last update time is close to 5 seconds. Otherwise, if the last update was a long time ago (30 secs, 5 minutes or more), then your spider has either stopped abnormally or hanged up. If the spider record has an end time then the crawler has finished correctly. List of Crawlers running simultaneously: your frontend can query for the records with an empty end time. Those spiders will be either running or dead (in case the last update time was a long time ago). Take in consideration that the spider_closed signal will not be called in case the process finish abruptly. You will need to have a cron job to cleanup and/or update the dead records. Don't forget to add the extension to your settings.py file, like: EXTENSIONS = { # SpiderDetails class is in the file mybot/extensions.py 'mybot.extensions.SpiderDetails': 1000, }",4,@Rho.. Thanks for the detailed info on developing an Extension.. I will follow the info and let you know on my progress.. Thanks.. – kishan Oct 11 '13 at 7:00,,,
scrapy crawler caught exception reading instance data,"I am new to python and want to use scrapy to build a web crawler. I go through the tutorial in http://blog.siliconstraits.vn/building-web-crawler-scrapy/. Spider code likes following: from scrapy.spider import BaseSpider from scrapy.selector import HtmlXPathSelector from nettuts.items import NettutsItem from scrapy.http import Request class MySpider(BaseSpider): name = ""nettuts"" allowed_domains = [""net.tutsplus.com""] start_urls = [""http://net.tutsplus.com/""] def parse(self, response): hxs = HtmlXPathSelector(response) titles = hxs.select('//h1[@class=""post_title""]/a/text()').extract() for title in titles: item = NettutsItem() item[""title""] = title yield item When launch the spider with command line: scrapy crawl nettus, it has following error: [boto] DEBUG: Retrieving credentials from metadata server. 2015-07-05 18:27:17 [boto] ERROR: Caught exception reading instance data Traceback (most recent call last): File ""/anaconda/lib/python2.7/site-packages/boto/utils.py"", line 210, in retry_url r = opener.open(req, timeout=timeout) File ""/anaconda/lib/python2.7/urllib2.py"", line 431, in open response = self._open(req, data) File ""/anaconda/lib/python2.7/urllib2.py"", line 449, in _open '_open', req) File ""/anaconda/lib/python2.7/urllib2.py"", line 409, in _call_chain result = func(*args) File ""/anaconda/lib/python2.7/urllib2.py"", line 1227, in http_open return self.do_open(httplib.HTTPConnection, req) File ""/anaconda/lib/python2.7/urllib2.py"", line 1197, in do_open raise URLError(err) URLError: <urlopen error [Errno 65] No route to host> 2015-07-05 18:27:17 [boto] ERROR: Unable to read instance data, giving up really do not know what's wrong. Hope somebody could help",10,Is that the full traceback (I'm guessing it's not)/,2 Answers 2,"in the settings.py file: add following code settings: DOWNLOAD_HANDLERS = {'s3': None,}",28,Where is this documented? – gusridd Oct 15 '15 at 3:34,The important information is: URLError: <urlopen error [Errno 65] No route to host> That is trying to tell you that your computer doesn't know how to communicate with the site you're trying to scrape. Are you able to access the site normally (i.e. in a web-browser) from the machine you're trying to run this python on?,0,"Well, I'm not sure how to help you further. All I can say is that ""No route to host"" is a networking problem telling you that the OS doesn't know how to send packets to the IP address it needs to send packets to. – CrazyCasta Jul 5 '15 at 16:54"
Scrapy restart crawler when crawling finished,When my Scrapy crawler has finished I would like to automatically start the same crawler again. Could this be done with a Scrapy function or do I have to use a Cronjob e.g. crontab?,1,So it isn't possible to check if the crawler has finished and start the batch again? Disabling the dupe filter would work but how to re-request the same page?,1 Answer 1,,,,Just a normal Scrapy run can't do this unless you disable the dupe filter and upon downloading a page re-request the same page. It's a bit of a hacky solution but technically it would work. Cronjob or a Bash script that runs in a loop would do the trick.,1,So it isn't possible to check if the crawler has finished and start the batch again? Disabling the dupe filter would work but how to re-request the same page? – Jesper1 Oct 23 '13 at 17:29
Running a Scrapy Crawler,"I am very new in Python and Scrapy and I have written a crawler in PyCharm as follow: import scrapy from scrapy.spiders import Spider from scrapy.http import Request import re class TutsplusItem(scrapy.Item): title = scrapy.Field() class MySpider(Spider): name = ""tutsplus"" allowed_domains = [""bbc.com""] start_urls = [""http://www.bbc.com/""] def parse(self, response): links = response.xpath('//a/@href').extract() # We stored already crawled links in this list crawledLinks = [] for link in links: # If it is a proper link and is not checked yet, yield it to the Spider #if linkPattern.match(link) and not link in crawledLinks: if not link in crawledLinks: link = ""http://www.bbc.com"" + link crawledLinks.append(link) yield Request(link, self.parse) titles = response.xpath('//a[contains(@class, ""media__link"")]/text()').extract() for title in titles: item = TutsplusItem() item[""title""] = title print(""Title is : %s"" %title) yield item However, when I run above codes, nothing prints on the screen! What is wrong in my code?",0,,3 Answers 3,,,,"You would typically start scrapy using scrapy crawl, which will hook everything up for you and start the crawling. It also looks like your code is not properly indented (only one line inside parse when they all should be).|Put the code in a text file, name it to something like your_spider.py and run the spider using the runspider command: scrapy runspider your_spider.py|To run a spider from within Pycharm you need to configure ""Run/Debug configuration"" properly. Running your_spider.py as a standalone script wouldn't result in anything. As mentioned by @stranac scrapy crawl is the way to go. With scrapy being a binary and crawl an argument of your binary. Configure Run/Debug In the main menu go to : Run > Run Configurations... Find the appropriate scrapy binary within your virtualenv and set its absolute path as Script. This should look like something like this: /home/username/.virtualenvs/your_virtualenv_name/bin/scrapy In Scrapy parameters set up the parameters the binary scrapy will execute. In your case, you wan to start your spider. this is how this should look like: crawl your_spider_name e.g. crawl tutsplus Make sure that the Python intrepreter is the one where you setup Scrapy and other packages needed for your project. Make sure that the working directory is the directory containing settings.pywhich is also generated by Scrapy. From now on you should be able to Run and Debug your spiders from within Pycharm.","0,0,0",",,"
Scrapy Crawler not following links,"I am writing a Scrapy crawler to scrape information from a property website, https://www.iproperty.com.sg/sale/?page=1, https://www.iproperty.com.sg/sale/?page=2 etc.. The idea is that for each row, obtain information from that row and make a request to a link on that row for further information. Once all rows on that page have been processed, move on to the next page and repeat: import scrapy from scrapy.linkextractors import LinkExtractor from scrapy.spiders import CrawlSpider, Rule from property.items import PropertyItem class IpropCrawlerSpider(CrawlSpider): name = 'iprop_crawler' allowed_domains = ['www.iproperty.com.sg'] start_urls = [""https://www.iproperty.com.sg/sale/?page=1""] rules = ( Rule(LinkExtractor(allow=r'sale\/\?page=[1-9]'), callback='parse_item', follow=True), ) def parse_item(self, response): prop_list_xpath = '//h3[@class=""cgiArp""]' for prop in response.xpath(prop_list_xpath): item = PropertyItem() item['name'] = prop.xpath('./a/text()').extract_first() deep_uri = prop.xpath('./a/@href').extract_first() deep_url = 'https://www.iproperty.com.sg' + deep_uri request = scrapy.Request(deep_url, callback=self.parse_per_prop) request.meta['item'] = item yield request def parse_per_prop(self, response): item = response.meta['item'] item['price'] = response\ .xpath('//div[@class=""property-price duzTnm""]/text()')\ .extract_first() item['address'] = response\ .xpath('//span[@class=""property-address sale-default""]/text()')\ .extract_first() item['property_type'] = response\ .xpath('//div[@class=""property-attr-propertyType cXGbLS""]' \ + '/div[2]/text()')\ .extract_first() yield item Running this crawler results in no data scraped: 2018-11-09 01:53:58 [scrapy.utils.log] INFO: Scrapy 1.5.1 started (bot: property) 2018-11-09 01:53:58 [scrapy.utils.log] INFO: Versions: lxml 3.7.2.0, libxml2 2.9.4, cssselect 1.0.0, parsel 1.5.0, w3lib 1.17.0, Twisted 17.1.0, Python 3.6.1 |Anaconda custom (64-bit)| (default, Mar 22 2017, 19:54:23) - [GCC 4.4.7 20120313 (Red Hat 4.4.7-1)], pyOpenSSL 16.2.0 (OpenSSL 1.0.2p 14 Aug 2018), cryptography 1.7.1, Platform Linux-4.18.16-arch1-1-ARCH-x86_64-with-arch 2018-11-09 01:53:58 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'property', 'DOWNLOAD_DELAY': 1, 'NEWSPIDER_MODULE': 'property.spiders', 'ROBOTSTXT_OBEY': True, 'SPIDER_MODULES': ['property.spiders']} 2018-11-09 01:53:58 [scrapy.middleware] INFO: Enabled extensions: ['scrapy.extensions.corestats.CoreStats', 'scrapy.extensions.telnet.TelnetConsole', 'scrapy.extensions.memusage.MemoryUsage', 'scrapy.extensions.logstats.LogStats'] 2018-11-09 01:53:58 [scrapy.middleware] INFO: Enabled downloader middlewares: ['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware', 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware', 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware', 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware', 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware', 'scrapy.downloadermiddlewares.retry.RetryMiddleware', 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware', 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware', 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware', 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware', 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware', 'scrapy.downloadermiddlewares.stats.DownloaderStats'] 2018-11-09 01:53:58 [scrapy.middleware] INFO: Enabled spider middlewares: ['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware', 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware', 'scrapy.spidermiddlewares.referer.RefererMiddleware', 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware', 'scrapy.spidermiddlewares.depth.DepthMiddleware'] 2018-11-09 01:53:58 [scrapy.middleware] INFO: Enabled item pipelines: [] 2018-11-09 01:53:58 [scrapy.core.engine] INFO: Spider opened 2018-11-09 01:53:58 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min) 2018-11-09 01:53:58 [scrapy.extensions.telnet] DEBUG: Telnet console listening on 127.0.0.1:6024 2018-11-09 01:53:58 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.iproperty.com.sg/robots.txt> (referer: None) 2018-11-09 01:54:01 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.iproperty.com.sg/sale/?page=1> (referer: None) 2018-11-09 01:54:01 [scrapy.core.engine] INFO: Closing spider (finished) 2018-11-09 01:54:01 [scrapy.statscollectors] INFO: Dumping Scrapy stats: {'downloader/request_bytes': 460, 'downloader/request_count': 2, 'downloader/request_method_count/GET': 2, 'downloader/response_bytes': 154841, 'downloader/response_count': 2, 'downloader/response_status_count/200': 2, 'finish_reason': 'finished', 'finish_time': datetime.datetime(2018, 11, 8, 17, 54, 1, 224281), 'log_count/DEBUG': 3, 'log_count/INFO': 7, 'memusage/max': 47136768, 'memusage/startup': 47136768, 'response_received_count': 2, 'scheduler/dequeued': 1, 'scheduler/dequeued/memory': 1, 'scheduler/enqueued': 1, 'scheduler/enqueued/memory': 1, 'start_time': datetime.datetime(2018, 11, 8, 17, 53, 58, 676635)} 2018-11-09 01:54:01 [scrapy.core.engine] INFO: Spider closed (finished) If I change parse_item to parse_start_url only the first page is scraped but the following links are not followed: 2018-11-09 02:11:42 [scrapy.statscollectors] INFO: Dumping Scrapy stats: {'downloader/request_bytes': 6195, 'downloader/request_count': 20, 'downloader/request_method_count/GET': 20, 'downloader/response_bytes': 2433163, 'downloader/response_count': 20, 'downloader/response_status_count/200': 20, 'finish_reason': 'shutdown', 'finish_time': datetime.datetime(2018, 11, 8, 18, 11, 42, 430358), 'item_scraped_count': 18, 'log_count/DEBUG': 39, 'log_count/INFO': 8, 'memusage/max': 47132672, 'memusage/startup': 47132672, 'request_depth_max': 1, 'response_received_count': 20, 'scheduler/dequeued': 19, 'scheduler/dequeued/memory': 19, 'scheduler/enqueued': 21, 'scheduler/enqueued/memory': 21, 'start_time': datetime.datetime(2018, 11, 8, 18, 11, 18, 416991)} 2018-11-09 02:11:42 [scrapy.core.engine] INFO: Spider closed (shutdown) I would like to seek enlightenment on this issue as to why I am unable to follow the link to the next pages.",2,"Unfortunately passing a string as an argument does not seem to change the result. Also, should it not be 'sale\/\?page=[1-9]\d*'?",2 Answers 2,So I discovered that there was a problem with the rule itself and had to use an xpath selector instead.,-1,,"Judging by the Scrapy documentation, it looks like your passing a reference to your parse_item method to the callback argument of the rule. However, according to the docs, this callback operates on the extracted links. That's not what you want because your function requires a Scrapy Response to run. So, what you should do is use the process_request argument. On a related note, I changed your regex because the way you have it now it'll only work for pages 1 to 9 rules = ( Rule(LinkExtractor(allow = r'sale\/\?page=[1-9]\d*'), process_request = 'parse_item', follow = True), ) As an aside, you probably shouldn't return a Request object back to Scrapy and instead should use scrapy.Item and ItemLoader to store your data.",2,"Unfortunately passing a string as an argument does not seem to change the result. Also, should it not be 'sale\/\?page=[1-9]\d*'? – Kevin Tham Nov 9 '18 at 17:02"
Scrapy Crawler in python cannot follow links?,"I wrote a crawler in python using the scrapy tool of python. The following is the python code: from scrapy.contrib.spiders import CrawlSpider, Rule from scrapy.contrib.linkextractors.sgml import SgmlLinkExtractor from scrapy.selector import HtmlXPathSelector #from scrapy.item import Item from a11ypi.items import AYpiItem class AYpiSpider(CrawlSpider): name = ""AYpi"" allowed_domains = [""a11y.in""] start_urls = [""http://a11y.in/a11ypi/idea/firesafety.html""] rules =( Rule(SgmlLinkExtractor(allow = ()) ,callback = 'parse_item') ) def parse_item(self,response): #filename = response.url.split(""/"")[-1] #open(filename,'wb').write(response.body) #testing codes ^ (the above) hxs = HtmlXPathSelector(response) item = AYpiItem() item[""foruri""] = hxs.select(""//@foruri"").extract() item[""thisurl""] = response.url item[""thisid""] = hxs.select(""//@foruri/../@id"").extract() item[""rec""] = hxs.select(""//@foruri/../@rec"").extract() return item But, instead of following the links the error thrown is: Traceback (most recent call last): File ""/usr/lib/python2.6/site-packages/Scrapy-0.12.0.2538-py2.6.egg/scrapy/cmdline.py"", line 131, in execute _run_print_help(parser, _run_command, cmd, args, opts) File ""/usr/lib/python2.6/site-packages/Scrapy-0.12.0.2538-py2.6.egg/scrapy/cmdline.py"", line 97, in _run_print_help func(*a, **kw) File ""/usr/lib/python2.6/site-packages/Scrapy-0.12.0.2538-py2.6.egg/scrapy/cmdline.py"", line 138, in _run_command cmd.run(args, opts) File ""/usr/lib/python2.6/site-packages/Scrapy-0.12.0.2538-py2.6.egg/scrapy/commands/crawl.py"", line 45, in run q.append_spider_name(name, **opts.spargs) --- <exception caught here> --- File ""/usr/lib/python2.6/site-packages/Scrapy-0.12.0.2538-py2.6.egg/scrapy/queue.py"", line 89, in append_spider_name spider = self._spiders.create(name, **spider_kwargs) File ""/usr/lib/python2.6/site-packages/Scrapy-0.12.0.2538-py2.6.egg/scrapy/spidermanager.py"", line 36, in create return self._spiders[spider_name](**spider_kwargs) File ""/usr/lib/python2.6/site-packages/Scrapy-0.12.0.2538-py2.6.egg/scrapy/contrib/spiders/crawl.py"", line 38, in __init__ self._compile_rules() File ""/usr/lib/python2.6/site-packages/Scrapy-0.12.0.2538-py2.6.egg/scrapy/contrib/spiders/crawl.py"", line 82, in _compile_rules self._rules = [copy.copy(r) for r in self.rules] exceptions.TypeError: 'Rule' object is not iterable Can someone please explain to me what's going on? Since this is the stuff mentioned in the documentation and I leave the allow field blank, that itself should make follow True by default. So why the error? What kind of optimisations can I make with my crawler to make it fast?",7,,1 Answer 1,,,,"From what I see, it looks like your rule is not an iterable. It looks like you were trying to make rules a tuple, you should read up on tuples in the python documentation. To fix your problem, change this line: rules =( Rule(SgmlLinkExtractor(allow = ()) ,callback = 'parse_item') ) To: rules =(Rule(SgmlLinkExtractor(allow = ()) ,callback = 'parse_item'),) Notice the comma at the end?",33,
Why scrapy crawler stops?,"I have written a crawler using scrapy framework to parse a products site. The crawler stops in between suddenly without completing the full parsing process. I have researched a lot on this and most of the answers indicate that my crawler is being blocked by the website. Is there any mechanism by which I can detect whether my spider is being stopped by website or does it stop on its own? The below is info level log entry of spider . 2013-09-23 09:59:07+0000 [scrapy] INFO: Scrapy 0.18.0 started (bot: crawler) 2013-09-23 09:59:08+0000 [spider] INFO: Spider opened 2013-09-23 09:59:08+0000 [spider] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min) 2013-09-23 10:00:08+0000 [spider] INFO: Crawled 10 pages (at 10 pages/min), scraped 7 items (at 7 items/min) 2013-09-23 10:01:08+0000 [spider] INFO: Crawled 22 pages (at 12 pages/min), scraped 19 items (at 12 items/min) 2013-09-23 10:02:08+0000 [spider] INFO: Crawled 31 pages (at 9 pages/min), scraped 28 items (at 9 items/min) 2013-09-23 10:03:08+0000 [spider] INFO: Crawled 40 pages (at 9 pages/min), scraped 37 items (at 9 items/min) 2013-09-23 10:04:08+0000 [spider] INFO: Crawled 49 pages (at 9 pages/min), scraped 46 items (at 9 items/min) 2013-09-23 10:05:08+0000 [spider] INFO: Crawled 59 pages (at 10 pages/min), scraped 56 items (at 10 items/min) Below is last part of debug level entry in log file before spider is closed: 2013-09-25 11:33:24+0000 [spider] DEBUG: Crawled (200) <GET http://url.html> (referer: http://site_name) 2013-09-25 11:33:24+0000 [spider] DEBUG: Scraped from <200 http://url.html> //scrapped data in json form 2013-09-25 11:33:25+0000 [spider] INFO: Closing spider (finished) 2013-09-25 11:33:25+0000 [spider] INFO: Dumping Scrapy stats: {'downloader/request_bytes': 36754, 'downloader/request_count': 103, 'downloader/request_method_count/GET': 103, 'downloader/response_bytes': 390792, 'downloader/response_count': 103, 'downloader/response_status_count/200': 102, 'downloader/response_status_count/302': 1, 'finish_reason': 'finished', 'finish_time': datetime.datetime(2013, 9, 25, 11, 33, 25, 1359), 'item_scraped_count': 99, 'log_count/DEBUG': 310, 'log_count/INFO': 14, 'request_depth_max': 1, 'response_received_count': 102, 'scheduler/dequeued': 100, 'scheduler/dequeued/disk': 100, 'scheduler/enqueued': 100, 'scheduler/enqueued/disk': 100, 'start_time': datetime.datetime(2013, 9, 25, 11, 23, 3, 869392)} 2013-09-25 11:33:25+0000 [spider] INFO: Spider closed (finished) Still there are pages remaining to be parsed, but the spider stops.",2,The crawler parses few product detail pages and stops in between without parsing all the product details pages.,1 Answer 1,,,,"So far I know that for a spider: There are some queue or pool of urls to be scraped/parsed with parsing methods. You can specify, bind the url to a specific method or let the default 'parse' do the job. From parsing methods you must return/yield another request(s), to feed that pool, or item(s) When the pool runs out of urls or a stop signal is sent the spider stops crawling. Would be nice if you share your spider code so we can check if those binds are correct. It's easy to miss some bindings by mistake using SgmlLinkExtractor for example.",0,I am trying this example mherman.org/blog/2012/11/08/… – Grahesh Parkar Sep 27 '13 at 5:19
my scrapy crawler doesnt return results from amazon.com,"Folks I have been coding Scrapy based web crawlers for a couple of weeks. They seem to be working as expected. I have become a scrapy fan. But in the last couple of days my latest scrapy crawler refuses to crawl the Amazon site. I dont get any results back. Neither do I get any error codes. I have even tried the scrapy shell. It just doesnt return any results. I suspect the problem is in the xpath or the css expression, but I'm not able to figure it out. Any help will be gladly appreciated. Here's what my spider looks like My code prints the xxxxx and nothing after import scrapy from scrapy.spiders import CrawlSpider, Rule from scrapy.linkextractors import LinkExtractor from amazon.items import LowesItem from amazon.items import SwatchcolorItem class SattySpider(scrapy.Spider): name = ""faucets"" allowed_domains = [""amazon.com""] start_urls = [ ""https://www.amazon.com/s?ie=UTF8&page=1&rh=n%3A228013%2Ck%3Abathroom%20faucets"" ] rules = ( Rule(LinkExtractor(allow='amazon\.com/[A-Z][a-zA-Z_/]+$'), 'parse_category', follow=True, ), ) def parse(self, response): print 'xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx' ##### # I even tried xpath #for sel in response.xpath('.//li[@class=""s-result-item celwidget s-hidden-sponsored-item""]'): # prodDesc= sel.xpath('.//div[@class=""s-item-container""]//div[@class=""a-row a-spacing-none""]//a[@title]').extract() ##### for sel in response.css(""li.s-result-item.celwidget.s-hidden-sponsored-item > div.s-item-container > div > div > a::attr('href')""): #for sel in response.xpath('.//li[@class=""s-result-item celwidget s-hidden-sponsored-item""]'): prodDesc= sel.xpath('.//div[@class=""s-item-container""]//div[@class=""a-row a-spacing-none""]//a[@title]').extract() print prodDesc produrls = sel.xpath('.//@data-producturl').extract() urls = sel.xpath('.//@data-productimg').extract() #prod_url_det = response.urljoin(produrl.extract()) lowi= LowesItem() lowi['swatcharray'] = {} for idx,swatch in enumerate(sel.xpath('.//div[@class=""product-container js-product-container""]//a//div[@class=""pvs pvs-options-height v-spacing-small""]//ul/li')): swatchcolor = swatch.xpath('.//img//@alt').extract() lowi['swatcharray'][idx] =swatchcolor #yield lowi #url_prod_det = response.urljoin(produrl) for idx1,url in enumerate(urls): url_prod_det = response.urljoin(produrls[idx1]) yield scrapy.Request(url_prod_det, meta={'lowes': LowesItem(prod=prod[idx1], swatcharray=lowi['swatcharray'], file_urls=['http:' + url])}, callback=self.parse_productdetail) for next in response.css(""div.grid-parent.v-spacing-extra-large > nav > ul > li.page-next > a::attr('href')""): url_next = response.urljoin(next.extract()) print "" url_next : "" + url_next yield scrapy.Request(url_next, callback=self.parse) def parse_productdetail(self, response): print 'Testing....' # for model in response.xpath('//div[@class=""pd-numbers grid-50 tablet-grid-100""]//p[@class=""secondary-text small-type""]').re('<strong> Model # </strong>'): for model in response.xpath('//div[@class=""pd-numbers grid-50 tablet-grid-100""]//p[@class=""secondary-text small-type""]'): #print model.extract() modelname = model.xpath('./text()').extract() #print modelname #yield lowesItem lowesItem = response.meta['lowes'] lowesItem['model']=modelname[1] lowesItem['category']='default' lowesItem['subcategory']='default' lowesItem['vendor']='Lowes' for namevals in response.xpath('//div[@id=""collapseSpecs""]//div[@class=""panel-body""]//div[@class=""grid-100 grid-parent""]//div[@class=""grid-50""]//table[@class=""table full-width no-borders""]//tbody//tr'): #print namevals name = namevals.xpath('.//th/text()').extract() val = namevals.xpath('.//td//span/text()').extract() if 'Faucet Type' in name: lowesItem['faucettype']=val[0] elif 'Number of Faucet Handles' in name: lowesItem['numofhandles']=val[0] elif 'ADA Compliant' in name: lowesItem['ada']=val[0] elif 'Built-In Water Filter' in name: lowesItem['builtinwaterfilter']=val[0] elif 'Mounting Location' in name: lowesItem['mountingloc']=val[0] elif 'Color/Finish Family' in name: lowesItem['color']=val[0] elif 'Manufacturer Color/Finish' in name: lowesItem['manufacturercolor']=val[0] elif 'Collection Name' in name: lowesItem['collection']=val[0] elif 'Soap or Lotion Dispenser' in name: lowesItem['soapdispenser']=val[0] elif 'Spout Height (Inches)' in name: lowesItem['spoutheight']=val[0] elif 'Max Flow Rate' in name: lowesItem['maxflowrate']=val[0] yield lowesItem",1,"dont scrape amazon, use the API",0,,,,,,
My Scrapy Crawler don't find nested a href tags,"I have written a Scrapy crawler as follow: import sys, getopt import scrapy from scrapy.spiders import Spider from scrapy.http import Request import re class TutsplusItem(scrapy.Item): title = scrapy.Field() class MySpider(Spider): name = ""tutsplus"" allowed_domains = [""bbc.com""] start_urls = [""http://www.bbc.com/""] crawling_level=None def __init__(self,crawling_level, *args): MySpider.crawling_level=crawling_level super(MySpider, self).__init__(self) def parse(self, response): links = response.xpath('//a/@href').extract() print(""Links are %s"" %links) print (""Crawling level is %s "" %MySpider.crawling_level ) # We stored already crawled links in this list level=MySpider.crawling_level crawledLinks = [] # Pattern to check proper link # I only want to get the tutorial posts # linkPattern = re.compile(""^\/tutorials\?page=\d+"") for link in links: # If it is a proper link and is not checked yet, yield it to the Spider #if linkPattern.match(link) and not link in crawledLinks: if not link in crawledLinks and level>0: link = ""http://www.bbc.com"" + link crawledLinks.append(link) yield Request(link, self.parse) titles = response.xpath('//a[contains(@class, ""media__link"")]/@*').extract() #titles = response.xpath('//a/@href').extract() print (""Titles are %s"" %titles ) count=0 for title in titles: item = TutsplusItem() item[""title""] = title print(""Title is : %s"" %title) yield item However, there is a problem in my codes and for the line titles = response.xpath('//a[contains(@class, ""media__link"")]').extract() it does not return any link. The HTML is as follow: <h3 class=""media__title""> <a class=""media__link"" href=""/news/world-us-canada-38965557"" rev=""hero1|headline"" > Trump adviser quits over Russia contacts</a> </h3> My output titles are always null. Is there anything wrong with my XPATH?",0,,1 Answer 1,,,,"The xpath in not correct ! use chrome dev tools for xpath debugging: ""//a[@class='media__link']/@href"" titles = response.xpath('//a[@class='media__link']/@href').extract()",0,
Scrapy crawler in Cron job,"I want to execute my scrapy crawler from cron job . i create bash file getdata.sh where scrapy project is located with it's spiders #!/bin/bash cd /myfolder/crawlers/ scrapy crawl my_spider_name My crontab looks like this , I want to execute it in every 5 minute */5 * * * * sh /myfolder/crawlers/getdata.sh but it don't works , whats wrong , where is my error ? when I execute my bash file from terminal sh /myfolder/crawlers/getdata.sh it works fine",19,"is the sh ""prefix"" in */5 * * * * sh /myfolder/crawlers/getdata.sh necessary to execute shell scripts from crontab???",7 Answers 7,I solved this problem including PATH into bash file #!/bin/bash cd /myfolder/crawlers/ PATH=$PATH:/usr/local/bin export PATH scrapy crawl my_spider_name,25,+1 Had the same problem and simply couldn't figure it out. You should mark your question as the accepted answer. :) – Xethron Sep 21 '13 at 9:47,"Adding the following lines in crontab -e runs my scrapy crawl at 5AM every day. This is a slightly modified version of crocs' answer PATH=/usr/bin * 5 * * * cd project_folder/project_name/ && scrapy crawl spider_name Without setting $PATH, cron would give me an error ""command not found: scrapy"". I guess this is because /usr/bin is where scripts to run programs are stored in Ubuntu. Note that the complete path for my scrapy project is /home/user/project_folder/project_name. I ran the env command in cron and noticed that the working directory is /home/user. Hence I skipped /home/user in my crontab above The cron log can be helpful while debugging grep CRON /var/log/syslog|Another option is to forget using a shell script and chain the two commands together directly in the cronjob. Just make sure the PATH variable is set before the first scrapy cronjob in the crontab list. Run: crontab -e to edit and have a look. I have several scrapy crawlers which run at various times. Some every 5 mins, others twice a day. PATH=/usr/local/bin */5 * * * * user cd /myfolder/crawlers/ && scrapy crawl my_spider_name_1 * 1,13 * * * user cd /myfolder/crawlers/ && scrapy crawl my_spider_name_2 All jobs located after the PATH variable will find scrapy. Here the first one will run every 5 mins and the 2nd twice a day at 1am and 1pm. I found this easier to manage. If you have other binaries to run then you may need to add their locations to the path.|For anyone who used pip3 (or similar) to install scrapy, here is a simple inline solution: */10 * * * * cd ~/project/path && ~/.local/bin/scrapy crawl something >> ~/crawl.log 2>&1 Replace: */10 * * * * with your cron pattern ~/project/path with the path to your scrapy project (where your scrapy.cfg is) something with the spider name (use scrapy list in your project to find out) ~/crawl.log with your log file position (in case you want to have logging)|Check where scrapy is installed using ""which scrapy"" command. In my case, scrapy is installed in /usr/local/bin. Open crontab for editing using crontab -e. PATH=$PATH:/usr/local/bin export PATH */5 * * * * cd /myfolder/path && scrapy crawl spider_name It should work. Scrapy runs every 5 minutes.|does your shell script have execute permission? e.g. can you do /myfolder/crawlers/getdata.sh without the sh? if you can then you can drop the sh in the line in cron|in my case scrapy is in .local/bin/scrapy give the proper path of scraper and name it worK perfect 0 0 * * * cd /home/user/scraper/Folder_of_scriper/ && /home/user/.local/bin/scrapy crawl ""name"" >> /home/user/scrapy.log 2>&1 /home/user/scrapy.log it use to save the output and error in scrapy.log for check it program work or not thank you.","9,3,2,1,0,0",",,where does the path ~/.local/bin/scrapy come from or what is the significance of it? – oldboy Jul 2 '18 at 0:57,,No it writes that permissions is denied – Beka Jun 21 '13 at 12:26,"
scrapyrt not receiving response from scrapy crawler,"I am trying to run scrapy crawler using scrapyrt. I get following response in browser {""status"": ""error"", ""message"": """", ""code"": 500} response: 1 and this one in scrapyrt window I have tried to edit the path of log file but it throws Permission denied error. The crawler runs successfully (as it creates html file) but not receiving json response in curl. $curl = curl_init(); curl_setopt_array($curl, array( CURLOPT_PORT=>'9080', CURLOPT_URL => ""http://localhost/crawl.json?spider_name=dmoz&url=http://www.dmoz.org/Computers/Programming/Languages/Ada/"", CURLOPT_FOLLOWLOCATION => true, CURLOPT_MAXREDIRS => 10, CURLOPT_USERAGENT => $_SERVER['HTTP_USER_AGENT'], CURLOPT_AUTOREFERER => true, CURLOPT_CONNECTTIMEOUT => 120, CURLOPT_TIMEOUT => 120, CURLOPT_POST => false )); $response = curl_exec($curl); $err = curl_error($curl); curl_close($curl); if ($err) { echo ""cURL Error #:"" . $err; } else { echo ""response: "".$response; } If the same crawler is executed from scapy cmd scrapy crawl dmoz -a url=""http://www.dmoz.org/Computers/Programming/Languages/Ada/"" the output is {'description': u'ACM Special Interest Group on Ada: information on SIGAda organization and pointers to current information and resources for the Ada programming language.', 'name': u'SIGAda', 'url': u'http://www.sigada.org/'}",0,,1 Answer 1,"Solved the issue: Updated ""C:\Python27\Lib\site-packages\scrapyrt\log.py"" file with following. Replaced filename = settings.get('LOG_FILE') with this filename = ""C:\\wamp64\\www\\dirbot-master\\logs\\dmoz\\log.log"" dirbot-master is scrapy project. Now I am receiving the response in browser.",1,,,,
Scrapy Crawler has me stumped,"I have a written a few scrapy crawlers. This one has me stumped. I'm scraping a faucet comapany's site. The layout is pretty simple. List page. Thumbnails on the list page take you to detail page. I get a bunch of info to be downloaded for every line item. I'm struggling to get the URL for the next page. I have done is a few times. The issue I'm running into is that my crawler isnt able to find the attribute href on a hyperlink marked by the a tag. It is able to find other attributes like ""data-bind"" but not href which is right there. Looks pretty straight forward & hence frustrating me all the more. In the code below the crawler is printing the amazon list as: [<Selector xpath='//div[@id=""product-list""]//div[@id=""displayedProducts""]//div[@data-bind=""attr: { id: \'prd\' + ModelName().replace(\'-\', \'\'), \'data-sortorder\': SortOrder, \'class\': \'product \' + ModelName().replace(\'-\', \'\') }""]//div[@class=""finishes""]//div[@data-bind=""attr: { \'class\': \'finishSwatch \' + FacetCode() }""]' data=u'<div data-bind=""attr: { \'class\': \'finish'>] and Pages as [] import scrapy from scrapy.spiders import CrawlSpider, Rule from scrapy.linkextractors import LinkExtractor from homedepot.items import LowesItem from homedepot.items import SwatchcolorItem class SattySpider(scrapy.Spider): name = ""satty-pfister"" allowed_domains = [""pfisterfaucets.com""] start_urls = [ ""http://www.pfisterfaucets.com/kitchen/category/kitchen-faucets"" ] rules = ( Rule(LinkExtractor(allow='pfisterfaucets\.com/[A-Z][a-zA-Z_/]+$'), 'parse_category', follow=True, ), ) def parse(self, response): print 'enterrrrrrrrrrrrr' #amazonlist = response.xpath('//ul[@id=""products""]//li[@class=""grid_3 ""]//div//a[@class=""preview_image js_product-link""]') amazonlist = response.xpath('//div[@id=""product-list""]//div[@id=""displayedProducts""]//div[@data-bind=""attr: { id: \'prd\' + ModelName().replace(\'-\', \'\'), \'data-sortorder\': SortOrder, \'class\': \'product \' + ModelName().replace(\'-\', \'\') }""]//div[@class=""finishes""]//div[@data-bind=""attr: { \'class\': \'finishSwatch \' + FacetCode() }""]') print amazonlist pages = amazonlist.xpath('.//a//@href').extract() print 'pages' print pages #imglarges = amazonlist.xpath('.//@srcset').extract() imgs = amazonlist.xpath('.//img//@src').extract() for idx1,page in enumerate(pages): print page url_next = response.urljoin(pages[idx1]) print url_next yield scrapy.Request(url_next,callback=self.parse_productdetail) for next_url in response.xpath('//div[@class=""float-right pagination-pages""]//div//a'): urls= next_url.xpath('.//@href').extract() for idx1,url in enumerate(urls): url_next = response.urljoin(urls[idx1]) print url_next yield scrapy.Request(url_next, callback=self.parse) def parse_productdetail(self, response): tits='blank' #<div class=""tabs-navigation group long""> #//div[contains(@class, ""tabs-navigation"")] for imgs in response.xpath('//div[contains(@class, ""tabs-navigation"")]//ul[@class=""tabs""]//li//img') : imgurl= imgs.xpath('.//@href').extract() name = imgs.xpath('.//@alt').extract() print 'imggggggggggggggggggggggggggggg' print imgurl lowesitem = LowesItem(prod=name, file_urls=[imgurl]) yield lowesitem",1,because there is AJAX items loading / scrapy doesn't execute js ... but anyway it's pretty easy to get them using plain requests (not xpath) - look through XHR requests in your browser network panel,0,,,,,,
How to use Scrapy Crawler with Splash to crawl Javascript pages,"I am having troubles using Scrapy Crawler to crawl javascript websites. It seems like Scrapy ignores Rules and just continues normal scraping. Would it be possible to instruct Spider to use Splash in order to crawl? Thank you. class MySpider(CrawlSpider): name = 'booki' start_urls = [ 'https://worldmap.com/listings/in/united-states/', ] rules = ( # Extract links matching 'category.php' (but not matching 'subsection.php') # and follow links from them (since no callback means follow=True by default). Rule(LinkExtractor(allow=('catalogue\/category', ), deny=('subsection\.php', ))), # Extract links matching 'item.php' and parse them with the spider's method parse_item Rule(LinkExtractor(allow=('catalogue', ),deny=('catalogue\/category')), callback='first_tier'), # ) custom_settings = { #'DOWNLOAD_DELAY' : '2', 'SPLASH_URL': 'http://localhost:8050', 'DOWNLOADER_MIDDLEWARES': { 'scrapy_splash.SplashCookiesMiddleware': 723, 'scrapy_splash.SplashMiddleware': 725, 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware': 810, }, 'SPIDER_MIDDLEWARES': { 'scrapy_splash.SplashDeduplicateArgsMiddleware': 100, }, 'DUPEFILTER_CLASS': 'scrapy_splash.SplashAwareDupeFilter', 'DOWNLOAD_DELAY' : '8', 'ITEM_PIPELINES' : { 'bookstoscrap.pipelines.BookstoscrapPipeline': 300, } } def start_requests(self): for url in self.start_urls: yield SplashRequest(url, self.first_tier, endpoint='render.html', args={'wait': 3.5}, )",0,Thank you again @malberts! Will try it out and keep you posted.,1 Answer 1,"The Rules will only trigger if you actually get to a matching page after the start_requests. You also need to define callback functions for your Rules, otherwise they will try to use the default parse (in case it appears as if your Rules are doing nothing). To change a Rule's request to SplashRequest you have to return it in the process_request callback. For example: class MySpider(CrawlSpider): # ... rules = ( Rule( LinkExtractor(allow=('catalogue\/category', ), deny=('subsection\.php', )), process_request='splash_request' ), Rule( LinkExtractor(allow=('catalogue', ), deny=('catalogue\/category'), callback='first_tier', process_request='splash_request' ), ) # ... def splash_request(self, request): return SplashRequest( request.url, callback=request.callback, endpoint='render.html', args={'wait': 3.5}, )",1,Thank you again @malberts! Will try it out and keep you posted. – Matija Žiberna Feb 19 '19 at 13:29,,,
Scrapy crawler acting weirdly,"I have written a script in python scrapy to parse different categories from craigslist. I noticed some weird things executing the script. It runs flawlessly leaving nothing to complaint. However, the thing is: if I leave items.py blank like below, it doesn't have any effect in the crawling process. My question is what is it doing in my scrapy project then? Thanks in advance. ""Items.py"" file contains: import scrapy class CraigItem(scrapy.Item): pass The spider contains: import scrapy from scrapy import Request class JobsSpider(scrapy.Spider): name = ""category"" allowed_domains = [""craigslist.org""] start_urls = [""https://newyork.craigslist.org/search/egr""] def parse(self, response): jobs = response.xpath('//p[@class=""result-info""]') for job in jobs: relative_url = job.xpath('a/@href').extract_first() absolute_url = response.urljoin(relative_url) title = job.xpath('a/text()').extract_first() address = job.xpath('span[@class=""result-meta""]/span[@class=""result-hood""]/text()').extract_first("""")[2:-1] yield Request(absolute_url, callback=self.parse_page, meta={'URL': absolute_url, 'Title': title, 'Address':address}) relative_next_url = response.xpath('//a[@class=""button next""]/@href').extract_first() absolute_next_url = ""https://newyork.craigslist.org"" + relative_next_url yield Request(absolute_next_url, callback=self.parse) def parse_page(self, response): url = response.meta.get('URL') title = response.meta.get('Title') address = response.meta.get('Address') compensation = response.xpath('//p[@class=""attrgroup""]/span[1]/b/text()').extract_first() employment_type = response.xpath('//p[@class=""attrgroup""]/span[2]/b/text()').extract_first() yield{'URL': url, 'Title': title, 'Address':address, 'Compensation':compensation, 'Employment_Type':employment_type} Again my question is: doesn't items.py file have any supervision in the crawling process? If it is, how?",0,"Thanks Tomáš Linhart, for your answer. In fact, I kicked items.py out from scrapy project and ran again. I found it still working. That means,, it is not being used by Scrapy.",1 Answer 1,"You should read about Scrapy Items first. In short, Scrapy Items are dict-like classes that define items your spider produces. When you yield an item from spider, it has to be either Scrapy Item or a dict (or, a Request object). In your spider, you chose to use the second approach, i.e. yield plain dict. The file items.py is a template produced by scrapy startproject command which defines blank Item class for you to enhance it, if you like. But as you don't use that class in your spider, it's not used by Scrapy.",1,"Thanks Tomáš Linhart, for your answer. In fact, I kicked items.py out from scrapy project and ran again. I found it still working. That means,, it is not being used by Scrapy. – SIM Sep 20 '17 at 13:38",,,
Running Scrapy crawler from a main function,"Ihave wriiten a crawler in scrapy but I would want to initiate the crwaling by using main method import sys, getopt import scrapy from scrapy.spiders import Spider from scrapy.http import Request import re class TutsplusItem(scrapy.Item): title = scrapy.Field() class MySpider(Spider): name = ""tutsplus"" allowed_domains = [""bbc.com""] start_urls = [""http://www.bbc.com/""] def __init__(self, *args): try: opts, args = getopt.getopt(args, ""hi:o:"", [""ifile="", ""ofile=""]) except getopt.GetoptError: print 'test.py -i <inputfile> -o <outputfile>' sys.exit(2) super(MySpider, self).__init__(self,*args) def parse(self, response): links = response.xpath('//a/@href').extract() # We stored already crawled links in this list crawledLinks = [] # Pattern to check proper link # I only want to get the tutorial posts # linkPattern = re.compile(""^\/tutorials\?page=\d+"") for link in links: # If it is a proper link and is not checked yet, yield it to the Spider #if linkPattern.match(link) and not link in crawledLinks: if not link in crawledLinks: link = ""http://www.bbc.com"" + link crawledLinks.append(link) yield Request(link, self.parse) titles = response.xpath('//a[contains(@class, ""media__link"")]/text()').extract() count=0 for title in titles: item = TutsplusItem() item[""title""] = title print(""Title is : %s"" %title) yield item Instead of using scrapy runspider Crawler.py arg1 arg2 I would like to have a seprate class with main function and initiate scrapy from there. How to this?",0,,1 Answer 1,,,,"There are different ways to approach this, but I suggest the following: Have a main.py file on the same directory that will open a new process and launch the spider with the parameters you need. The main.py file would have something like the following: import subprocess scrapy_command = 'scrapy runspider {spider_name} -a param_1=""{param_1}""'.format(spider_name='your_spider', param_1='your_value') process = subprocess.Popen(scrapy_command, shell=True) With this code, you just need to call your main file. python main.py Hope it helps.",0,
Scrapy: How to get processed pipeline items from crawler?,I created a Scrapy Crawler with several pipeline steps. The Crawler is part of a bigger framework which requires the crawler to return a list of the parsed items. In Scrapy I implemented a pipeline containing several normalization steps. As Scrapy is part of the bigger framework - it would be great if I could return the items to the crawler and/or the framework after passed through the whole pipeline. Is there a way to accomplish this? E.g. some pseudo code url = 'http://somewebsite.com' crawler = MyCrawler(url) # each parsed website passes each pipeline step and is yielded / returned. all_items_from_pipeline = crawler.run(),0,Isn't it possible to get a deferred object from the crawler?,1 Answer 1,,,,"You can not do this the functional way, because Scrapy is asynchronous. But you can save the items in a file or database and then the other component can take them from there. You also get the benefit that the items can be stored there until needed.",0,Isn't it possible to get a deferred object from the crawler? – Jon Dec 12 '13 at 9:19
Connect Scrapy crawler with S3,"My crawler download from a URL a Request.body which I save on a file locally. Now I would like to connect to my aws-s3. I read the documentation but face two issues: 1. the config as well as the credential files are not of a dict type? my file is an unmodified was-credential and aws-config files. The s3 config key is not a dictionary type, ignoring its value of: None the Response is a 'bytes' one and cannot not be processed by the feeder as such. I tried Response.text and then got the same error raised but with 'str'. Any help is highly appreciated. Thank you. Additional information: config file (path ~/.aws/config): [default] Region=eu-west-2 output=csv and credentials file (path ~/.aws/credentials): [default] aws_access_key_id=aws_access_key_id=foo aws_secret_access_key=aws_secret_access_key=bar the link to Scrapy documentation: https://docs.scrapy.org/en/latest/topics/settings.html?highlight=s3 MacBook-Pro:aircraftPositions frederic$ scrapy crawl aircraftData_2018 2019-03-13 15:35:04 [scrapy.utils.log] INFO: Scrapy 1.6.0 started (bot: aircraftPositions) 2019-03-13 15:35:04 [scrapy.utils.log] INFO: Versions: lxml 4.3.1.0, libxml2 2.9.9, cssselect 1.0.3, parsel 1.5.1, w3lib 1.20.0, Twisted 18.9.0, Python 3.7.1 (v3.7.1:260ec2c36a, Oct 20 2018, 03:13:28) - [Clang 6.0 (clang-600.0.57)], pyOpenSSL 19.0.0 (OpenSSL 1.1.1a 20 Nov 2018), cryptography 2.5, Platform Darwin-18.2.0-x86_64-i386-64bit 2019-03-13 15:35:04 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'aircraftPositions', 'CONCURRENT_REQUESTS': 32, 'CONCURRENT_REQUESTS_PER_DOMAIN': 32, 'DOWNLOAD_DELAY': 10, 'FEED_FORMAT': 'json', 'FEED_STORE_EMPTY': True, 'FEED_URI': 's3://flightlists/lists_v1/%(name)s/%(time)s.json', 'NEWSPIDER_MODULE': 'aircraftPositions.spiders', 'ROBOTSTXT_OBEY': True, 'SPIDER_MODULES': ['aircraftPositions.spiders'], 'USER_AGENT': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/55.0.2883.87 Safari/537.36'} 2019-03-13 15:35:04 [scrapy.extensions.telnet] INFO: Telnet Password: 2f2c11f3300481ed 2019-03-13 15:35:04 [py.warnings] WARNING: /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/scrapy/utils/misc.py:144: ScrapyDeprecationWarning: Initialising `scrapy.extensions.feedexport.S3FeedStorage` without AWS keys is deprecated. Please supply credentials or use the `from_crawler()` constructor. return objcls(*args, **kwargs) 2019-03-13 15:35:04 [botocore.hooks] DEBUG: Changing event name from creating-client-class.iot-data to creating-client-class.iot-data-plane 2019-03-13 15:35:04 [botocore.hooks] DEBUG: Changing event name from before-call.apigateway to before-call.api-gateway 2019-03-13 15:35:04 [botocore.hooks] DEBUG: Changing event name from request-created.machinelearning.Predict to request-created.machine-learning.Predict 2019-03-13 15:35:04 [botocore.hooks] DEBUG: Changing event name from before-parameter-build.autoscaling.CreateLaunchConfiguration to before-parameter-build.auto-scaling.CreateLaunchConfiguration 2019-03-13 15:35:04 [botocore.hooks] DEBUG: Changing event name from before-parameter-build.route53 to before-parameter-build.route-53 2019-03-13 15:35:04 [botocore.hooks] DEBUG: Changing event name from request-created.cloudsearchdomain.Search to request-created.cloudsearch-domain.Search 2019-03-13 15:35:04 [botocore.hooks] DEBUG: Changing event name from docs.*.autoscaling.CreateLaunchConfiguration.complete-section to docs.*.auto-scaling.CreateLaunchConfiguration.complete-section 2019-03-13 15:35:04 [botocore.hooks] DEBUG: Changing event name from before-parameter-build.logs.CreateExportTask to before-parameter-build.cloudwatch-logs.CreateExportTask 2019-03-13 15:35:04 [botocore.hooks] DEBUG: Changing event name from docs.*.logs.CreateExportTask.complete-section to docs.*.cloudwatch-logs.CreateExportTask.complete-section 2019-03-13 15:35:04 [botocore.hooks] DEBUG: Changing event name from before-parameter-build.cloudsearchdomain.Search to before-parameter-build.cloudsearch-domain.Search 2019-03-13 15:35:04 [botocore.hooks] DEBUG: Changing event name from docs.*.cloudsearchdomain.Search.complete-section to docs.*.cloudsearch-domain.Search.complete-section 2019-03-13 15:35:04 [botocore.loaders] DEBUG: Loading JSON file: /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/botocore/data/endpoints.json 2019-03-13 15:35:04 [botocore.hooks] DEBUG: Event choose-service-name: calling handler <function handle_service_name_alias at 0x103ca4b70> 2019-03-13 15:35:04 [botocore.loaders] DEBUG: Loading JSON file: /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/botocore/data/s3/2006-03-01/service-2.json 2019-03-13 15:35:04 [botocore.hooks] DEBUG: Event creating-client-class.s3: calling handler <function add_generate_presigned_post at 0x103c688c8> 2019-03-13 15:35:04 [botocore.hooks] DEBUG: Event creating-client-class.s3: calling handler <function add_generate_presigned_url at 0x103c686a8> 2019-03-13 15:35:04 [botocore.args] DEBUG: The s3 config key is not a dictionary type, ignoring its value of: None 2019-03-13 15:35:04 [botocore.endpoint] DEBUG: Setting s3 timeout as (60, 60) 2019-03-13 15:35:04 [botocore.loaders] DEBUG: Loading JSON file: /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/botocore/data/_retry.json 2019-03-13 15:35:04 [botocore.client] DEBUG: Registering retry handlers for service: s3 2019-03-13 15:35:04 [botocore.client] DEBUG: Defaulting to S3 virtual host style addressing with path style addressing fallback. 2019-03-13 15:35:04 [scrapy.middleware] INFO: Enabled extensions: ['scrapy.extensions.corestats.CoreStats', 'scrapy.extensions.telnet.TelnetConsole', 'scrapy.extensions.memusage.MemoryUsage', 'scrapy.extensions.feedexport.FeedExporter', 'scrapy.extensions.logstats.LogStats'] 2019-03-13 15:35:04 [scrapy.middleware] INFO: Enabled downloader middlewares: ['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware', ... after that it is enabling the downloader and the middleware. This is the spider: class QuotesSpider(scrapy.Spider): name = ""aircraftData_2018"" def url_values(self): time = list(range(1538140980, 1538140780, -60)) return time def start_requests(self): allowed_domains = [""https://domaine.net""] list_urls = [] for n in self.url_values(): list_urls.append(""https://domaine.net/.../.../all/{}"".format(n)) for url in list_urls: yield scrapy.Request(url=url, callback=self.parse, dont_filter=True) def parse(self, response): i = AircraftpositionsItem i['url'] = response.url i['body'] = response.body yield I This is the pipeline.py class AircraftpositionsPipeline(object): def process_item(self, item, spider): return item def return_body(self, response): page = response.url.split(""/"")[-1] filename = 'aircraftList-{}.csv'.format(page) with open(filename, 'wb') as f: f.write(response.body)",2,"It would help if you could provide following: - link to a tutorial you're reading - screenshot of file with your credentials that you're using (cover actual values, I don't need your credentials, I just would need to see file format) - piece of your python code where you use those credentials",0,,,,,,
Improving this Scrapy crawler,"I've written a web crawler which extracts absolute links from the starting URL, and keeps visiting absolute links within the domain until it stops. Scrapy automatically doesn't follow duplicate links. The crawler works. import scrapy class TestSpider(scrapy.Spider): name = 'test_spider' start_urls = ['http://homeguide.ph/'] allowed_domains = ['homeguide.ph'] # for the initial visit def parse(self, response): links = response.xpath('//a/@href').extract() for link in links: if link.find(""#"") == -1: yield scrapy.Request(link, callback=self.parse_link) # for subsequent visits def parse_link(self, response): self.logger.info(""Visited %s"", response.url) links = response.xpath('//a/@href').extract() for link in links: if link.find(""#"") == -1: # visit only absolute links yield scrapy.Request(link, callback=self.parse_link) I feel like it can be improved, though. I'm not sure how. Is there a way to improve this crawler?",-2,,1 Answer 1,,,,1) You can use CrawlSpider object. It extract all links and follow it automatically. You can check it here,2,
Scrapy - Broad crawler for domain collecting,"I'm trying to build a Scrapy crawler for domain collecting. What I want is to collect websites with '.sk' or '.cz' suffixes. My idea is to give a list of start urls (1000 sk and cz urls) and crawler would go to url like www.example.com, check for links, if these links has the correct suffixes, then yield Request objects of these urls - basic forms. So crawler goes to example.com, it extracts two links: https://www.page.cz/about and https://www.liss.cz/info/45 and yield two requests with these urls: https://www.page.cz and https://www.liss.cz. I've created a simple spider now but It doesn't find any links and ends. Do you have any ideas to improve this spider? class doSpider(CrawlSpider): name = ""do_crawler"" start_urls = [ ""https://www.seznam.cz/"", ] def __init__(self, *args, **kwargs): self.extractor = LxmlLinkExtractor(allow='/.cz|.sk/') super(doSpider, self).__init__(*args, **kwargs) def start_requests(self): for url in self.start_urls: yield scrapy.Request(url, callback=self.parse_links, meta={}) def parse_links(self, response): for url in self.extractor.extract_links(response): # parse and save to db yield scrapy.Request(url, callback=self.parse_links, meta={})",1,I don't think it's the right place for code review and code improvement discussions. Maybe try code review exchange. General suggestions I can give is to have errbacks for your broad-crawl requests and check out broad-crawl tips over official docs,0,,,,,,
Scrapy crawler not recursively crawling next page,"I am trying to build this crawler to get housing data from craigslist, but the crawler stops after fetching the first page and does not go to the next page . Here is the code , it works for the first page ,but for the love of god I dont understand why it does not get to the next page .Any insight is really appreciated .I followed this part from scrapy tutorial import scrapy import re from scrapy.linkextractors import LinkExtractor class QuotesSpider(scrapy.Spider): name = ""craigslistmm"" start_urls = [ ""https://vancouver.craigslist.ca/search/hhh"" ] def parse_second(self,response): #need all the info in a dict meta_dict = response.meta for q in response.css(""section.page-container""): meta_dict[""post_details""]= { ""location"": {""longitude"":q.css(""div.mapAndAttrs div.mapbox div.viewposting::attr(data-longitude)"" ).extract(), ""latitude"":q.css(""div.mapAndAttrs div.mapbox div.viewposting::attr(data-latitude)"" ).extract()}, ""detailed_info"": ' '.join(q.css('section#postingbody::text').extract()).strip() } return meta_dict def parse(self, response): pattern = re.compile(""\/([a-z]+)\/([a-z]+)\/.+"") for q in response.css(""li.result-row""): post_urls = q.css(""p.result-info a::attr(href)"").extract_first() mm = re.match(pattern, post_urls) neighborhood= q.css(""p.result-info span.result-meta span.result-hood::text"").extract_first() next_url = ""https://vancouver.craigslist.ca/""+ post_urls request = scrapy.Request(next_url,callback=self.parse_second) #next_page = response.xpath('.//a[@class=""button next""]/@href').extract_first() #follow_url = ""https://vancouver.craigslist.ca/"" + next_page #request1 = scrapy.Request(follow_url,callback=self.parse) #yield response.follow(next_page,callback = self.parse) request.meta['id'] = q.css(""li.result-row::attr(data-pid)"").extract_first() request.meta['pricevaluation'] = q.css(""p.result-info span.result-meta span.result-price::text"").extract_first() request.meta[""information""] = q.css(""p.result-info span.result-meta span.housing::text"" ).extract_first() request.meta[""neighborhood""] =q.css(""p.result-info span.result-meta span.result-hood::text"").extract_first() request.meta[""area""] = mm.group(1) request.meta[""adtype""] = mm.group(2) yield request #yield scrapy.Request(follow_url, callback=self.parse) next_page = LinkExtractor(allow=""s=\d+"").extract_links(response)[0] # = ""https://vancouver.craigslist.ca/"" + next_page yield response.follow(next_page.url,callback=self.parse)",0,"there is only a single link that will be fetched by this ,I have tried diff way to build the next_page ,similar to what you mentioned ,but It did not work .",1 Answer 1,,,,"The problem seems to be with the next_page extraction using LinkExtractor. If you look in the look, you'll see duplicate requests being filtered. There are more links on the page that satisfy your extraction rule and maybe they are not extracted in any particular order (or not in the order you wish). I think better approach is to extract exactly the information you want, try it with this: next_page = response.xpath('//span[@class=""buttons""]//a[contains(., ""next"")]/@href').extract_first()",0,"there is only a single link that will be fetched by this ,I have tried diff way to build the next_page ,similar to what you mentioned ,but It did not work . – Bg1850 Aug 25 '17 at 5:01"
Aliexpress.com Scrapy Crawler localStorage.x5referer,"I need to crawl aliexpress search result data related to my search keyword inputs. Below my Scrapy code sample. I got localStorage.x5referer script in loging file. Maybe i need to handle some cookie things. # -*- coding: utf-8 -*- import scrapy class AliexpresSpider(scrapy.Spider): name = 'aliexpres' allowed_domains = ['aliexpress.com'] start_urls = ['https://tr.aliexpress.com/af/sehpa.html?SearchText=sehpa'] def parse(self,response): print(response.text) # srplists = response.css('ul.list-items') # for item in srplists.css('li.list-item'): # title = item.css('div.item-title-wrap::text').get() # price = item.css('div.item-price-wrap > span.price-big-sale::text').get() # url = item.css('a ::attr(href)').get() # yield {'title':title,'price':price,'url':url}",0,"Hi, I didn't handle this with Scrapy. But you can try crawl Aliexpress with Selenium. Doc: [selenium-python.readthedocs.io/]",1 Answer 1,,,,Any findings on the topic ? I had the same problem there.,0,"Hi, I didn't handle this with Scrapy. But you can try crawl Aliexpress with Selenium. Doc: [selenium-python.readthedocs.io/] – tugrulv Feb 1 at 12:06"
Optimisation Crawler Scrapy,"I'm using scrapy to find expired domains, my crawler crawl the web and add every externals domains to the database (MySql) and after I check the availability with a PHP script. The database have around 300k domains and now the crawler is very slow because I check before each insert if the domain is not into the database yet with this request SQL request of the insert_table() method : sql = ""INSERT INTO %s (url) SELECT * FROM (SELECT '%s') AS tmp WHERE NOT EXISTS (SELECT url FROM website WHERE url = '%s' ) LIMIT 1"" % (SQL_TABLE, datas, datas) The Crawler : class HttpbinSpider(CrawlSpider): name = ""expired"" start_urls = [ 'http://mywebsite.com', ] custom_settings = { 'RETRY_ENABLED': True, 'DEPTH_LIMIT' : 0, 'DEPTH_PRIORITY' : 1, 'LOG_ENABLED' : False, 'CONCURRENT_REQUESTS_PER_DOMAIN' : 32, 'CONCURRENT_REQUESTS' : 64, } rules = (Rule(LxmlLinkExtractor(allow=()), callback='parse_obj', follow=True),) def parse_obj(self,response): item = MyItem() item['url'] = [] for link in LxmlLinkExtractor(allow=('.com', '.fr', '.net', '.org', '.info'), deny=('facebook', 'amazon', 'wordpress'),).extract_links(response): ext = tldextract.extract(link.url) insert_table(ext.registered_domain) Can someone help me to find solutions to get expired domains why my crawler and keep the best performances. Thank's",0,Do you have an index on the url column?,1 Answer 1,,,,You have really bad sql query. Add a unique key for a column url and ignore on duplicate that will speed up inserting. Unique index will work for you. Select is unnecessary.,0,
Putting arguments for my crawler in scrapy,"I have written a scrapy crawler but I need to add the ability to read some arguments from the command line and then populates some static fields in my spider class. I also need to override the initialiser so it populates some of the spider fields. import scrapy from scrapy.spiders import Spider from scrapy.http import Request import re class TutsplusItem(scrapy.Item): title = scrapy.Field() class MySpider(Spider): name = ""tutsplus"" allowed_domains = [""bbc.com""] start_urls = [""http://www.bbc.com/""] def parse(self, response): links = response.xpath('//a/@href').extract() # We stored already crawled links in this list crawledLinks = [] for link in links: # If it is a proper link and is not checked yet, yield it to the Spider # if linkPattern.match(link) and not link in crawledLinks: if not link in crawledLinks: link = ""http://www.bbc.com"" + link crawledLinks.append(link) yield Request(link, self.parse) titles = response.xpath('//a[contains(@class, ""media__link"")]/text()').extract() for title in titles: item = TutsplusItem() item[""title""] = title print(""Title is : %s"" % title) yield item Then it should be run as: scrapy runspider crawler.py arg1 arg2 How do I achieve this?",-1,"thanks for the response but what if I want to run like ""scrapy runspider Crawler.py arg1 arg2"", for example using ""getopt.getopt(args, options, [long_options])""",1 Answer 1,,,,"You can do that by overriding the init method of your spider like this. class MySpider(Spider): name = ""tutsplus"" allowed_domains = [""bbc.com""] start_urls = [""http://www.bbc.com/""] arg1 = None arg2 = None def __init__(self, arg1, arg2, *args, **kwargs): self.arg1 = arg1 self.arg2 = arg2 super(MySpider, self).__init__(*args, **kwargs) def parse(self, response): links = response.xpath('//a/@href').extract() # We stored already crawled links in this list crawledLinks = [] for link in links: # If it is a proper link and is not checked yet, yield it to the Spider # if linkPattern.match(link) and not link in crawledLinks: if not link in crawledLinks: link = ""http://www.bbc.com"" + link crawledLinks.append(link) yield Request(link, self.parse) titles = response.xpath('//a[contains(@class, ""media__link"")]/text()').extract() for title in titles: item = TutsplusItem() item[""title""] = title print(""Title is : %s"" % title) yield item Then run your spider like scrapy crawl tutsplus -a arg1=arg1 -a arg2=arg2",0,"thanks for the response but what if I want to run like ""scrapy runspider Crawler.py arg1 arg2"", for example using ""getopt.getopt(args, options, [long_options])"" – Mahdi Feb 13 '17 at 9:18"
How to keep Scrapy Crawler running,"I currently have a Scrapy crawler that runs once. I've been searching for a solution to have it continuously repeat its crawling cycle until it's stopped. In other words, once the first iteration of the crawl completes, automatically start a second iteration without stopping the entire crawler, after that a third iteration, and so on. Also, perhaps running again after x seconds, although I'm unsure how the system would react in the case of the previous crawl process not finishing while also trying to launch another iteration. Solutions I've found online thus far only refer to cron or scrapyd which I'm not interested in. I'm more interested in implementing a custom scheduler within the crawler project using processes such as CrawlerRunner or reactors. Does anyone have a couple pointers? The following code from another stackoverflow question is the closest information I found in regard to my questions, but am looking for some advice on how to implement a more continuous approach. + from twisted.internet import reactor, defer + from scrapy.crawler import CrawlerRunner + from scrapy.utils.log import configure_logging + def run_crawl(): + """""" + Run a spider within Twisted. Once it completes, + wait 5 seconds and run another spider. + """""" + runner = CrawlerRunner(get_project_settings()) + runner.crawl(SpiderA) + runner.crawl(SpiderB) + deferred = runner.join() + deferred.addCallback(reactor.callLater, 5, run_crawl) + return deferred + run_crawl() + reactor.run() Error: ""message"": ""Module 'twisted.internet.reactor' has no 'run' member"", ""source"": ""pylint"", UPDATE How to schedule Scrapy crawl execution programmatically Tried to implement this but am unable to import my spider, I get module not found error. Also the reactor variables are red with error and say Module 'twisted.internet.reactor' has no 'callLater' member//////or has no 'run' member.",0,"You failed to link to the question you copied the code from, but it is very suspicious that your run_crawl returns a deferred that gets thrown away",2 Answers 2,,,,"Unless you ellaborate on what you mean by “more continuous”, the only way I can think of to make the code of the quoted response more continuous is to replace 5 with 0 in the deferred.|Use apscheduler # -*- coding: utf-8 -*- from scrapy.crawler import CrawlerProcess from scrapy.utils.project import get_project_settings from apscheduler.schedulers.twisted import TwistedScheduler from Demo.spiders.google import GoogleSpider # your spider process = CrawlerProcess(get_project_settings()) scheduler = TwistedScheduler() scheduler.add_job(process.crawl, 'interval', args=[GoogleSpider], seconds=10) scheduler.start() process.start(False)","0,0","I stumbled upon this stackoverflow question stackoverflow.com/questions/47552507/… – buklaou Jan 21 '19 at 21:32,When answering an old question, or any question in general, it would be great if you could provide some context to your answer rather than mostly code. – David Buck Nov 24 '19 at 10:50"
How Do You Pass New URLs to a Scrapy Crawler,"I would like to keep a scrapy crawler constantly running inside a celery task worker probably using something like this. Or as suggested in the docs The idea would be to use the crawler for querying an external API returning XML responses. I would like to pass the URL (or query parameters and let the crawler build the URL) I want to query to the crawler, and the crawler would make the URL call, and give me back the extracted items. How can I pass this new URL I want to fetch to the crawler once it started running. I do not want to restart the crawler every time I want to give it a new URL, instead I want the crawler to sit idly waiting for URLs to crawl. The two methods I've spotted to run scrapy inside another python process use a new Process to run the crawler in. I would like to not have to fork and teardown a new process every time I want to crawl a URL, since that is pretty expensive and unnecessary.",1,"Yes, I had considered something like that, even using github.com/darkrho/scrapy-redis but I was planning on running the crawler itself as a celery task, which I think would be easier to manage. I may have to give it some more thought, whether having it poll redis ontop of running inside Celery is too much potential for a clusterfluff. The main reason I'd like to keep Celery is because of the many tools to manage workers and create workflows (like the canvas). So any ideas for the original question?",2 Answers 2,,,,"Just have a spider that polls a database (or file?) that when presented with a new URL creates and yields a new Request() object for it. You can build it by hand easily enough. There is probably a better way to do it than that, but thats basically what I did for an open-proxy scraper. The spider gets a list of all the 'potential' proxies from the database and generates a Request() object for each one - when they're returned they're then dispatched down the chain and verified by downstream middleware and their records are updated by item pipeline.|You could use a message queue (like IronMQ--full disclosure, I work for the company that makes IronMQ as a developer evangelist) to pass in the URLs. Then in your crawler, poll for the URLs from the queue, and crawl based on the messages you retrieve. The example you linked to could be updated (this is untested and pseudocode, but you should get the basic idea): from twisted.internet import reactor from scrapy.crawler import Crawler from scrapy.settings import Settings from scrapy import log from testspiders.spiders.followall import FollowAllSpider from iron-mq import IronMQ mq = IronMQ() q = mq.queue(""scrape_queue"") crawler = Crawler(Settings()) crawler.configure() while True: # poll forever msg = q.get(timeout=120) # get messages from queue # timeout is the number of seconds the message will be reserved for, making sure no other crawlers get that message. Set it to a safe value (the max amount of time it will take you to crawl a page) if len(msg[""messages""]) < 1: # if there are no messages waiting to be crawled time.sleep(1) # wait one second continue # try again spider = FollowAllSpider(domain=msg[""messages""][0][""body""]) # crawl the domain in the message crawler.crawl(spider) crawler.start() log.start() reactor.run() # the script will block here q.delete(msg[""messages""][0][""id""]) # when you're done with the message, delete it","0,0","Yes, I had considered something like that, even using github.com/darkrho/scrapy-redis but I was planning on running the crawler itself as a celery task, which I think would be easier to manage. I may have to give it some more thought, whether having it poll redis ontop of running inside Celery is too much potential for a clusterfluff. The main reason I'd like to keep Celery is because of the many tools to manage workers and create workflows (like the canvas). So any ideas for the original question? – Andres May 23 '13 at 4:22,what file would you call the spider from? – Adders May 17 '16 at 15:56"
Scrapy crawler spider doesn't follow links,"For this, I used example in Scrapy crawl spider example: http://doc.scrapy.org/en/latest/topics/spiders.html I want to get links from a web page and follow them to parse table with statistics, but somehow I don't see that any links would be grabbed and followed to web page that has data. Here is my script: from basketbase.items import BasketbaseItem from scrapy.contrib.spiders import CrawlSpider, Rule from scrapy.contrib.linkextractors.sgml import SgmlLinkExtractor from scrapy.selector import HtmlXPathSelector from scrapy.http import Request class Basketspider(CrawlSpider): name = ""basketsp"" allowed_domains = [""euroleague.net""] start_urls = [""http://www.euroleague.net/main""] rules = ( Rule(SgmlLinkExtractor(allow=(""results/by-date?seasoncode=E2000"")),follow=True), Rule(SgmlLinkExtractor(allow=(""showgame?gamecode=165&seasoncode=E2000#!boxscore"")), callback='parse_item'), ) def parse_item(self, response): self.log('Hi, this is an item page! %s' % response.url) sel = HtmlXPathSelector(response) items=[] item = BasketbaseItem() item['date'] = sel.select('//div[@class=""gs-dates""]/text()').extract() # Game date item['time'] = sel.select('//div[@class=""gs-dates""]/span[@class=""GameScoreTimeContainer""]/text()').extract() # Game time item['stage'] = sel.select('//div[@class=""gs-dates""]/text()').extract() # Stage of tournament item['home'] = sel.select('//div[@class=""gs-teams""]/a[@class=""localClub""]/text()').extract() #Home team item['guest'] = sel.select('//div[@class=""gs-teams""]/a[@class=""roadClub""]/text()').extract() #Visitor team item['referees'] = sel.select('//span[@id=""ctl00_ctl00_ctl00_ctl00_maincontainer_maincenter_contentpane_boxscorepane_ctl00_lblReferees""]/text()').extract() #Referees item['attendance'] = sel.select('//span[@id=""ctl00_ctl00_ctl00_ctl00_maincontainer_maincenter_contentpane_boxscorepane_ctl00_lblAudience""]/text()').extract() item['fst'] = sel.select('//table[@id=""ctl00_ctl00_ctl00_ctl00_maincontainer_maincenter_contentpane_boxscorepane_ctl00_PartialsStatsByQuarter_dgPartials""]//tr[2]/td[2][@class=""AlternatingColumn""]/text()').extract()+sel.select('//table[@id=""ctl00_ctl00_ctl00_ctl00_maincontainer_maincenter_contentpane_boxscorepane_ctl00_PartialsStatsByQuarter_dgPartials""]//tr[3]/td[2][@class=""AlternatingColumn""]/text()').extract() item['snd'] = sel.select('//table[@id=""ctl00_ctl00_ctl00_ctl00_maincontainer_maincenter_contentpane_boxscorepane_ctl00_PartialsStatsByQuarter_dgPartials""]//tr[2]/td[3][@class=""NormalColumn""]/text()').extract()+sel.select('//table[@id=""ctl00_ctl00_ctl00_ctl00_maincontainer_maincenter_contentpane_boxscorepane_ctl00_PartialsStatsByQuarter_dgPartials""]//tr[3]/td[3][@class=""NormalColumn""]/text()').extract() item['trd'] = sel.select('//table[@id=""ctl00_ctl00_ctl00_ctl00_maincontainer_maincenter_contentpane_boxscorepane_ctl00_PartialsStatsByQuarter_dgPartials""]//tr[2]/td[4][@class=""AlternatingColumn""]/text()').extract()+sel.select('//table[@id=""ctl00_ctl00_ctl00_ctl00_maincontainer_maincenter_contentpane_boxscorepane_ctl00_PartialsStatsByQuarter_dgPartials""]//tr[3]/td[4][@class=""AlternatingColumn""]/text()').extract() item['tth'] = sel.select('//table[@id=""ctl00_ctl00_ctl00_ctl00_maincontainer_maincenter_contentpane_boxscorepane_ctl00_PartialsStatsByQuarter_dgPartials""]//tr[2]/td[5][@class=""NormalColumn""]/text()').extract()+sel.select('//table[@id=""ctl00_ctl00_ctl00_ctl00_maincontainer_maincenter_contentpane_boxscorepane_ctl00_PartialsStatsByQuarter_dgPartials""]//tr[3]/td[5][@class=""NormalColumn""]/text()').extract() item['xt1'] = sel.select('//div[@class=""gs-dates""]/text()').extract() item['xt2'] = sel.select('//div[@class=""gs-dates""]/text()').extract() item['xt3'] = sel.select('//div[@class=""gs-dates""]/text()').extract() item['xt4'] = sel.select('//div[@class=""gs-dates""]/text()').extract() item['game_id'] = sel.select('//span[@id=""ctl00_ctl00_ctl00_ctl00_maincontainer_maincenter_contentpane_boxscorepane_ctl00_lblReferees""]/text()').extract() # Game ID construct item['arena'] = sel.select('//div[@class=""gs-dates""]/text()').extract() #Arena item['result'] = sel.select('//span[@class=""score""]/text()').extract() #Result item['league'] = sel.select('//div[@class=""gs-dates""]/text()').extract() #League print item['date'],item['time'], item['stage'], item['home'],item['guest'],item['referees'],item['attendance'],item['fst'],item['snd'],item['trd'],item['tth'],item['result'] items.append(item) And here I have response from terminal: scrapy crawl basketsp 2013-11-17 01:40:15+0200 [scrapy] INFO: Scrapy 0.16.2 started (bot: basketbase) 2013-11-17 01:40:15+0200 [scrapy] DEBUG: Enabled extensions: LogStats, TelnetConsole, CloseSpider, WebService, CoreStats, SpiderState 2013-11-17 01:40:15+0200 [scrapy] DEBUG: Enabled downloader middlewares: HttpAuthMiddleware, DownloadTimeoutMiddleware, UserAgentMiddleware, RetryMiddleware, DefaultHeadersMiddleware, RedirectMiddleware, CookiesMiddleware, HttpCompressionMiddleware, ChunkedTransferMiddleware, DownloaderStats 2013-11-17 01:40:15+0200 [scrapy] DEBUG: Enabled spider middlewares: HttpErrorMiddleware, OffsiteMiddleware, RefererMiddleware, UrlLengthMiddleware, DepthMiddleware 2013-11-17 01:40:15+0200 [scrapy] DEBUG: Enabled item pipelines: 2013-11-17 01:40:15+0200 [basketsp] INFO: Spider opened 2013-11-17 01:40:15+0200 [basketsp] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min) 2013-11-17 01:40:15+0200 [scrapy] DEBUG: Telnet console listening on 0.0.0.0:6023 2013-11-17 01:40:15+0200 [scrapy] DEBUG: Web service listening on 0.0.0.0:6080 2013-11-17 01:40:15+0200 [basketsp] DEBUG: Crawled (200) <GET http://www.euroleague.net/main> (referer: None) 2013-11-17 01:40:15+0200 [basketsp] INFO: Closing spider (finished) 2013-11-17 01:40:15+0200 [basketsp] INFO: Dumping Scrapy stats: {'downloader/request_bytes': 228, 'downloader/request_count': 1, 'downloader/request_method_count/GET': 1, 'downloader/response_bytes': 9018, 'downloader/response_count': 1, 'downloader/response_status_count/200': 1, 'finish_reason': 'finished', 'finish_time': datetime.datetime(2013, 11, 16, 23, 40, 15, 496752), 'log_count/DEBUG': 7, 'log_count/INFO': 4, 'response_received_count': 1, 'scheduler/dequeued': 1, 'scheduler/dequeued/memory': 1, 'scheduler/enqueued': 1, 'scheduler/enqueued/memory': 1, 'start_time': datetime.datetime(2013, 11, 16, 23, 40, 15, 229125)} 2013-11-17 01:40:15+0200 [basketsp] INFO: Spider closed (finished) What I am doing, wrong here? Any ideas would be great help. I tried to leave SgmlLinkExtractor() empty that all links would be followed, but I get the same situation. There's no indication that crawler spider works at all. I'm running Scrapy version 0.16.2 on Python 2.7.2+",1,the start_url is maybe too AJAXy for scrapy. try putting this link instead: euroleague.net/main/results/by-date,2 Answers 2,"Scrapy is misinterpreting the content type of the start url. You can verify this by using scrapy shell: $ scrapy shell 'http://www.euroleague.net/main' 2013-11-18 16:39:26+0900 [scrapy] INFO: Scrapy 0.21.0 started (bot: scrapybot) ... AttributeError: 'Response' object has no attribute 'body_as_unicode' See my previous answer about the missing body_as_unicode attribute. I notice that the server does not set any content-type header. CrawlSpider ignores non-html responses, so the responses are not processed and no links are followed. I would suggest opening a issue on github, as I think Scrapy should be able to handle this case transparently. As a work around you could override the CrawlSpider parse method, create an HtmlResponse from the response object passed, and pass that to the superclass parse method.",2,"Nailed the problem. I think the problem might be kind of at both end the website and scrapy too. The web site is not sending any regular content-type headers in response, which scrapy decide to figure out the response type. Falling back this scrapy has a method to check for the response body, but for some reason it is not getting used. – Biswanath Nov 18 '13 at 12:09","prepend ""www"" to allowed domains.",-1,No it doesn't work and from documentation you can see in it is not used there: doc.scrapy.org/en/latest/topics/spiders.html – Vy.Iv Nov 17 '13 at 22:07
